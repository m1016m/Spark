{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis （有时被称为意见挖掘或情绪人工智能）是指使用自然语言处理、文本分析、计算语言学和生物特征来系统地识别、提取、量化和研究情感状态和主观信息。情感分析广泛应用于客户的声音材料，如评论和调查回复、在线和社交媒体，以及医疗保健材料，应用范围从市场营销到客户服务再到临床医学。\n",
    "\n",
    "一般来说，情绪分析的目的是 决定态度 指演说者、作家或其他主题关于某个主题或整个语境的极性或对某个文档、互动或事件的情感反应。态度可以是一种判断或评价（见评价理论）、情感状态（即作者或演讲者的情感状态）或预期的情感交流（即作者或对话者预期的情感效果）。\n",
    "\n",
    "商业中的情绪分析，也称为意见挖掘，是一个根据文本所传达的语调来识别和编目文本的过程。它具有广泛的应用：\n",
    "\n",
    "商务智能构建中的情绪分析\n",
    "\n",
    "企业竞争优势的情绪分析\n",
    "\n",
    "通过业务中的情绪分析提升客户体验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#情绪分析管道\n",
    "#设置Spark上下文和SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark Sentiment Analysis example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#负载数据集\n",
    "df = spark.read.format('com.databricks.spark.csv').\\\n",
    "                               options(header='true', \\\n",
    "                               inferschema='true').\\\n",
    "            load(\"file:///home/hadoop/newtwitter.csv\",header=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import re\n",
    "\n",
    "# remove non ASCII characters\n",
    "def strip_non_ascii(data_str):\n",
    "    ''' Returns the string without non ASCII characters'''\n",
    "    stripped = (c for c in data_str if 0 < ord(c) < 127)\n",
    "    return ''.join(stripped)\n",
    "# setup pyspark udf function\n",
    "strip_non_ascii_udf = udf(strip_non_ascii, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------+--------------------+\n",
      "|                text|        id|pubdate|       text_non_asci|\n",
      "+--------------------+----------+-------+--------------------+\n",
      "|10 Things Missing...|2602860537|  18536|10 Things Missing...|\n",
      "|RT @_NATURALBWINN...|2602850443|  18536|RT @_NATURALBWINN...|\n",
      "|RT @HBO24 yo the ...|2602761852|  18535|RT @HBO24 yo the ...|\n",
      "|Aaaaaaaand I have...|2602738438|  18535|Aaaaaaaand I have...|\n",
      "|can I please have...|2602684185|  18535|can I please have...|\n",
      "+--------------------+----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#检查：\n",
    "df = df.withColumn('text_non_asci',strip_non_ascii_udf(df['text']))\n",
    "df.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_abbreviation(data_str):\n",
    "    data_str = data_str.lower()\n",
    "    data_str = re.sub(r'\\bthats\\b', 'that is', data_str)\n",
    "    data_str = re.sub(r'\\bive\\b', 'i have', data_str)\n",
    "    data_str = re.sub(r'\\bim\\b', 'i am', data_str)\n",
    "    data_str = re.sub(r'\\bya\\b', 'yeah', data_str)\n",
    "    data_str = re.sub(r'\\bcant\\b', 'can not', data_str)\n",
    "    data_str = re.sub(r'\\bdont\\b', 'do not', data_str)\n",
    "    data_str = re.sub(r'\\bwont\\b', 'will not', data_str)\n",
    "    data_str = re.sub(r'\\bid\\b', 'i would', data_str)\n",
    "    data_str = re.sub(r'wtf', 'what the fuck', data_str)\n",
    "    data_str = re.sub(r'\\bwth\\b', 'what the hell', data_str)\n",
    "    data_str = re.sub(r'\\br\\b', 'are', data_str)\n",
    "    data_str = re.sub(r'\\bu\\b', 'you', data_str)\n",
    "    data_str = re.sub(r'\\bk\\b', 'OK', data_str)\n",
    "    data_str = re.sub(r'\\bsux\\b', 'sucks', data_str)\n",
    "    data_str = re.sub(r'\\bno+\\b', 'no', data_str)\n",
    "    data_str = re.sub(r'\\bcoo+\\b', 'cool', data_str)\n",
    "    data_str = re.sub(r'rt\\b', '', data_str)\n",
    "    data_str = data_str.strip()\n",
    "    return data_str\n",
    "\n",
    "fix_abbreviation_udf = udf(fix_abbreviation, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------+--------------------+--------------------+\n",
      "|                text|        id|pubdate|       text_non_asci|        fixed_abbrev|\n",
      "+--------------------+----------+-------+--------------------+--------------------+\n",
      "|10 Things Missing...|2602860537|  18536|10 Things Missing...|10 things missing...|\n",
      "|RT @_NATURALBWINN...|2602850443|  18536|RT @_NATURALBWINN...|@_naturalbwinner ...|\n",
      "|RT @HBO24 yo the ...|2602761852|  18535|RT @HBO24 yo the ...|@hbo24 yo the #ne...|\n",
      "|Aaaaaaaand I have...|2602738438|  18535|Aaaaaaaand I have...|aaaaaaaand i have...|\n",
      "|can I please have...|2602684185|  18535|can I please have...|can i please have...|\n",
      "+--------------------+----------+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('fixed_abbrev',fix_abbreviation_udf(df['text_non_asci']))\n",
    "df.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除不相关的功能\n",
    "def remove_features(data_str):\n",
    "    # compile regex\n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    mention_re = re.compile('@(\\w+)')\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "    # convert to lowercase\n",
    "    data_str = data_str.lower()\n",
    "    # remove hyperlinks\n",
    "    data_str = url_re.sub(' ', data_str)\n",
    "    # remove @mentions\n",
    "    data_str = mention_re.sub(' ', data_str)\n",
    "    # remove puncuation\n",
    "    data_str = punc_re.sub(' ', data_str)\n",
    "    # remove numeric 'words'\n",
    "    data_str = num_re.sub(' ', data_str)\n",
    "    # remove non a-z 0-9 characters and words shorter than 1 characters\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    for word in data_str.split():\n",
    "        if list_pos == 0:\n",
    "            if alpha_num_re.match(word) and len(word) > 1:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = ' '\n",
    "        else:\n",
    "            if alpha_num_re.match(word) and len(word) > 1:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            else:\n",
    "                cleaned_str += ' '\n",
    "        list_pos += 1\n",
    "   #刪除不需要的空格，* .split（）將自動拆分\n",
    "   #whitespace和discard duplicates，“”。join（）加入\n",
    "   #結果列表成一個字符串。\n",
    "    return \" \".join(cleaned_str.split())\n",
    "# setup pyspark udf function\n",
    "remove_features_udf = udf(remove_features, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------+--------------------+--------------------+--------------------+\n",
      "|                text|        id|pubdate|       text_non_asci|        fixed_abbrev|             removed|\n",
      "+--------------------+----------+-------+--------------------+--------------------+--------------------+\n",
      "|10 Things Missing...|2602860537|  18536|10 Things Missing...|10 things missing...|things missing in...|\n",
      "|RT @_NATURALBWINN...|2602850443|  18536|RT @_NATURALBWINN...|@_naturalbwinner ...|oh and do not lik...|\n",
      "|RT @HBO24 yo the ...|2602761852|  18535|RT @HBO24 yo the ...|@hbo24 yo the #ne...|yo the newtwitter...|\n",
      "|Aaaaaaaand I have...|2602738438|  18535|Aaaaaaaand I have...|aaaaaaaand i have...|aaaaaaaand have t...|\n",
      "|can I please have...|2602684185|  18535|can I please have...|can i please have...|can please have t...|\n",
      "+--------------------+----------+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('removed',remove_features_udf(df['fixed_abbrev']))\n",
    "df.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#情绪分析主要功能\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "sentiment_analysis_udf = udf(sentiment_analysis , FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------+--------------------+--------------------+--------------------+---------------+\n",
      "|                text|        id|pubdate|       text_non_asci|        fixed_abbrev|             removed|sentiment_score|\n",
      "+--------------------+----------+-------+--------------------+--------------------+--------------------+---------------+\n",
      "|10 Things Missing...|2602860537|  18536|10 Things Missing...|10 things missing...|things missing in...|    -0.03181818|\n",
      "|RT @_NATURALBWINN...|2602850443|  18536|RT @_NATURALBWINN...|@_naturalbwinner ...|oh and do not lik...|    -0.03181818|\n",
      "|RT @HBO24 yo the ...|2602761852|  18535|RT @HBO24 yo the ...|@hbo24 yo the #ne...|yo the newtwitter...|      0.3181818|\n",
      "|Aaaaaaaand I have...|2602738438|  18535|Aaaaaaaand I have...|aaaaaaaand i have...|aaaaaaaand have t...|     0.11818182|\n",
      "|can I please have...|2602684185|  18535|can I please have...|can i please have...|can please have t...|     0.13636364|\n",
      "+--------------------+----------+-------+--------------------+--------------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df  = df.withColumn(\"sentiment_score\", sentiment_analysis_udf( df['removed'] ))\n",
    "df.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#情绪分类\n",
    "def condition(r):\n",
    "    if (r >=0.1):\n",
    "        label = \"positive\"\n",
    "    elif(r <= -0.1):\n",
    "        label = \"negative\"\n",
    "    else:\n",
    "        label = \"neutral\"\n",
    "    return label\n",
    "\n",
    "sentiment_udf = udf(lambda x: condition(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每个情绪班的头条微博\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
