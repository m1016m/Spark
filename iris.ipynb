{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#决策树分类器\n",
    "# 我们以iris数据集（iris）为例进行分析。iris以鸢尾花的特征作为数据来源，数据集包含150个数据集，分为3类，每类50个数据，每个数据包含4个属性\n",
    "# ，是在数据挖掘、数据分类中非常常用的测试集、训练集。决策树可以用于分类和回归\n",
    "from pyspark.ml.linalg import Vector,Vectors\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer,VectorIndexer\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:[5.1,3.5,1.4,0.2]\n",
      "0:[4.9,3.0,1.4,0.2]\n",
      "0:[4.7,3.2,1.3,0.2]\n",
      "0:[4.6,3.1,1.5,0.2]\n",
      "0:[5.0,3.6,1.4,0.2]\n",
      "0:[5.4,3.9,1.7,0.4]\n",
      "0:[4.6,3.4,1.4,0.3]\n",
      "0:[5.0,3.4,1.5,0.2]\n",
      "0:[4.4,2.9,1.4,0.2]\n",
      "0:[4.9,3.1,1.5,0.1]\n",
      "0:[5.4,3.7,1.5,0.2]\n",
      "0:[4.8,3.4,1.6,0.2]\n",
      "0:[4.8,3.0,1.4,0.1]\n",
      "0:[4.3,3.0,1.1,0.1]\n",
      "0:[5.8,4.0,1.2,0.2]\n",
      "0:[5.7,4.4,1.5,0.4]\n",
      "0:[5.4,3.9,1.3,0.4]\n",
      "0:[5.1,3.5,1.4,0.3]\n",
      "0:[5.7,3.8,1.7,0.3]\n",
      "0:[5.1,3.8,1.5,0.3]\n",
      "0:[5.4,3.4,1.7,0.2]\n",
      "0:[5.1,3.7,1.5,0.4]\n",
      "0:[4.6,3.6,1.0,0.2]\n",
      "0:[5.1,3.3,1.7,0.5]\n",
      "0:[4.8,3.4,1.9,0.2]\n",
      "0:[5.0,3.0,1.6,0.2]\n",
      "0:[5.0,3.4,1.6,0.4]\n",
      "0:[5.2,3.5,1.5,0.2]\n",
      "0:[5.2,3.4,1.4,0.2]\n",
      "0:[4.7,3.2,1.6,0.2]\n",
      "0:[4.8,3.1,1.6,0.2]\n",
      "0:[5.4,3.4,1.5,0.4]\n",
      "0:[5.2,4.1,1.5,0.1]\n",
      "0:[5.5,4.2,1.4,0.2]\n",
      "0:[4.9,3.1,1.5,0.1]\n",
      "0:[5.0,3.2,1.2,0.2]\n",
      "0:[5.5,3.5,1.3,0.2]\n",
      "0:[4.9,3.1,1.5,0.1]\n",
      "0:[4.4,3.0,1.3,0.2]\n",
      "0:[5.1,3.4,1.5,0.2]\n",
      "0:[5.0,3.5,1.3,0.3]\n",
      "0:[4.5,2.3,1.3,0.3]\n",
      "0:[4.4,3.2,1.3,0.2]\n",
      "0:[5.0,3.5,1.6,0.6]\n",
      "0:[5.1,3.8,1.9,0.4]\n",
      "0:[4.8,3.0,1.4,0.3]\n",
      "0:[5.1,3.8,1.6,0.2]\n",
      "0:[4.6,3.2,1.4,0.2]\n",
      "0:[5.3,3.7,1.5,0.2]\n",
      "0:[5.0,3.3,1.4,0.2]\n",
      "1:[7.0,3.2,4.7,1.4]\n",
      "1:[6.4,3.2,4.5,1.5]\n",
      "1:[6.9,3.1,4.9,1.5]\n",
      "1:[5.5,2.3,4.0,1.3]\n",
      "1:[6.5,2.8,4.6,1.5]\n",
      "1:[5.7,2.8,4.5,1.3]\n",
      "1:[6.3,3.3,4.7,1.6]\n",
      "1:[4.9,2.4,3.3,1.0]\n",
      "1:[6.6,2.9,4.6,1.3]\n",
      "1:[5.2,2.7,3.9,1.4]\n",
      "1:[5.0,2.0,3.5,1.0]\n",
      "1:[5.9,3.0,4.2,1.5]\n",
      "1:[6.0,2.2,4.0,1.0]\n",
      "1:[6.1,2.9,4.7,1.4]\n",
      "1:[5.6,2.9,3.6,1.3]\n",
      "1:[6.7,3.1,4.4,1.4]\n",
      "1:[5.6,3.0,4.5,1.5]\n",
      "1:[5.8,2.7,4.1,1.0]\n",
      "1:[6.2,2.2,4.5,1.5]\n",
      "1:[5.6,2.5,3.9,1.1]\n",
      "1:[5.9,3.2,4.8,1.8]\n",
      "1:[6.1,2.8,4.0,1.3]\n",
      "1:[6.3,2.5,4.9,1.5]\n",
      "1:[6.1,2.8,4.7,1.2]\n",
      "1:[6.4,2.9,4.3,1.3]\n",
      "1:[6.6,3.0,4.4,1.4]\n",
      "1:[6.8,2.8,4.8,1.4]\n",
      "1:[6.7,3.0,5.0,1.7]\n",
      "1:[6.0,2.9,4.5,1.5]\n",
      "1:[5.7,2.6,3.5,1.0]\n",
      "1:[5.5,2.4,3.8,1.1]\n",
      "1:[5.5,2.4,3.7,1.0]\n",
      "1:[5.8,2.7,3.9,1.2]\n",
      "1:[6.0,2.7,5.1,1.6]\n",
      "1:[5.4,3.0,4.5,1.5]\n",
      "1:[6.0,3.4,4.5,1.6]\n",
      "1:[6.7,3.1,4.7,1.5]\n",
      "1:[6.3,2.3,4.4,1.3]\n",
      "1:[5.6,3.0,4.1,1.3]\n",
      "1:[5.5,2.5,4.0,1.3]\n",
      "1:[5.5,2.6,4.4,1.2]\n",
      "1:[6.1,3.0,4.6,1.4]\n",
      "1:[5.8,2.6,4.0,1.2]\n",
      "1:[5.0,2.3,3.3,1.0]\n",
      "1:[5.6,2.7,4.2,1.3]\n",
      "1:[5.7,3.0,4.2,1.2]\n",
      "1:[5.7,2.9,4.2,1.3]\n",
      "1:[6.2,2.9,4.3,1.3]\n",
      "1:[5.1,2.5,3.0,1.1]\n",
      "1:[5.7,2.8,4.1,1.3]\n",
      "2:[6.3,3.3,6.0,2.5]\n",
      "2:[5.8,2.7,5.1,1.9]\n",
      "2:[7.1,3.0,5.9,2.1]\n",
      "2:[6.3,2.9,5.6,1.8]\n",
      "2:[6.5,3.0,5.8,2.2]\n",
      "2:[7.6,3.0,6.6,2.1]\n",
      "2:[4.9,2.5,4.5,1.7]\n",
      "2:[7.3,2.9,6.3,1.8]\n",
      "2:[6.7,2.5,5.8,1.8]\n",
      "2:[7.2,3.6,6.1,2.5]\n",
      "2:[6.5,3.2,5.1,2.0]\n",
      "2:[6.4,2.7,5.3,1.9]\n",
      "2:[6.8,3.0,5.5,2.1]\n",
      "2:[5.7,2.5,5.0,2.0]\n",
      "2:[5.8,2.8,5.1,2.4]\n",
      "2:[6.4,3.2,5.3,2.3]\n",
      "2:[6.5,3.0,5.5,1.8]\n",
      "2:[7.7,3.8,6.7,2.2]\n",
      "2:[7.7,2.6,6.9,2.3]\n",
      "2:[6.0,2.2,5.0,1.5]\n",
      "2:[6.9,3.2,5.7,2.3]\n",
      "2:[5.6,2.8,4.9,2.0]\n",
      "2:[7.7,2.8,6.7,2.0]\n",
      "2:[6.3,2.7,4.9,1.8]\n",
      "2:[6.7,3.3,5.7,2.1]\n",
      "2:[7.2,3.2,6.0,1.8]\n",
      "2:[6.2,2.8,4.8,1.8]\n",
      "2:[6.1,3.0,4.9,1.8]\n",
      "2:[6.4,2.8,5.6,2.1]\n",
      "2:[7.2,3.0,5.8,1.6]\n",
      "2:[7.4,2.8,6.1,1.9]\n",
      "2:[7.9,3.8,6.4,2.0]\n",
      "2:[6.4,2.8,5.6,2.2]\n",
      "2:[6.3,2.8,5.1,1.5]\n",
      "2:[6.1,2.6,5.6,1.4]\n",
      "2:[7.7,3.0,6.1,2.3]\n",
      "2:[6.3,3.4,5.6,2.4]\n",
      "2:[6.4,3.1,5.5,1.8]\n",
      "2:[6.0,3.0,4.8,1.8]\n",
      "2:[6.9,3.1,5.4,2.1]\n",
      "2:[6.7,3.1,5.6,2.4]\n",
      "2:[6.9,3.1,5.1,2.3]\n",
      "2:[5.8,2.7,5.1,1.9]\n",
      "2:[6.8,3.2,5.9,2.3]\n",
      "2:[6.7,3.3,5.7,2.5]\n",
      "2:[6.7,3.0,5.2,2.3]\n",
      "2:[6.3,2.5,5.0,1.9]\n",
      "2:[6.5,3.0,5.2,2.0]\n",
      "2:[6.2,3.4,5.4,2.3]\n",
      "2:[5.9,3.0,5.1,1.8]\n"
     ]
    }
   ],
   "source": [
    "#读取文本文件，第一个map把每行的数据用“,”隔开，比如在我们的数据集中，每行被分成了5部分，前4部分是鸢尾花的4个特征，最后一部分是鸢尾花的\n",
    "#分类；我们这里把特征存储在Vector中，创建一个Iris模式的RDD，然后转化成dataframe；然后把刚刚得到的数据注册成一个表iris，注册成这个表\n",
    "#之后，我们就可以通过sql语句进行数据查询\n",
    "def f(x):\n",
    "    rel = {}\n",
    "    rel['features'] = Vectors.dense(float(x[0]),float(x[1]),float(x[2]),float(x[3]))\n",
    "    rel['label'] = str(x[4])\n",
    "    return rel\n",
    " \n",
    "data = spark.sparkContext.textFile(\"file:///home/hadoop/iris.txt\").map(lambda line: line.split(',')).map(lambda p: Row(**f(p))).toDF()\n",
    " \n",
    "data.createOrReplaceTempView(\"iris\")\n",
    " \n",
    "df = spark.sql(\"select * from iris\")\n",
    " \n",
    "rel = df.rdd.map(lambda t : str(t[1])+\":\"+str(t[0])).collect()\n",
    "for item in rel:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分别获取标签列和特征列，进行索引，并进行了重命名。\n",
    "labelIndexer = StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\").fit(df)\n",
    " \n",
    "featureIndexer = VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").setMaxCategories(4).fit(df)\n",
    " \n",
    "#这里我们设置一个labelConverter，目的是把预测的类别重新转化成字符型的。\n",
    "labelConverter = IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)\n",
    "#接下来，我们把数据集随机分成训练集和测试集，其中训练集占70%。\n",
    "trainingData, testData = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-----------------+\n",
      "|predictedLabel|label|         features|\n",
      "+--------------+-----+-----------------+\n",
      "|             0|    0|[4.4,2.9,1.4,0.2]|\n",
      "|             0|    0|[4.4,3.0,1.3,0.2]|\n",
      "|             0|    0|[4.7,3.2,1.6,0.2]|\n",
      "|             0|    0|[4.8,3.4,1.9,0.2]|\n",
      "|             0|    0|[5.0,3.0,1.6,0.2]|\n",
      "|             0|    0|[5.0,3.2,1.2,0.2]|\n",
      "|             0|    0|[5.0,3.3,1.4,0.2]|\n",
      "|             0|    0|[5.0,3.4,1.6,0.4]|\n",
      "|             0|    0|[5.0,3.5,1.3,0.3]|\n",
      "|             1|    1|[5.1,2.5,3.0,1.1]|\n",
      "|             0|    0|[5.1,3.4,1.5,0.2]|\n",
      "|             0|    0|[5.1,3.5,1.4,0.3]|\n",
      "|             0|    0|[5.1,3.7,1.5,0.4]|\n",
      "|             0|    0|[5.1,3.8,1.5,0.3]|\n",
      "|             0|    0|[5.2,3.4,1.4,0.2]|\n",
      "|             0|    0|[5.4,3.4,1.7,0.2]|\n",
      "|             1|    1|[5.5,2.5,4.0,1.3]|\n",
      "|             2|    2|[5.6,2.8,4.9,2.0]|\n",
      "|             0|    0|[5.7,3.8,1.7,0.3]|\n",
      "|             1|    1|[5.8,2.6,4.0,1.2]|\n",
      "+--------------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#导入所需要的包\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel,DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "#训练决策树模型,这里我们可以通过setter的方法来设置决策树的参数，也可以用ParamMap来设置（具体的可以查看spark mllib的官网）。具体的可以设置的参数可以通过explainParams()来获取。\n",
    "dtClassifier = DecisionTreeClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\")\n",
    "#在pipeline中进行设置\n",
    "pipelinedClassifier = Pipeline().setStages([labelIndexer, featureIndexer, dtClassifier, labelConverter])\n",
    "#训练决策树模型\n",
    "modelClassifier = pipelinedClassifier.fit(trainingData)\n",
    "#进行预测\n",
    "predictionsClassifier = modelClassifier.transform(testData)\n",
    "#查看部分预测的结果\n",
    "predictionsClassifier.select(\"predictedLabel\", \"label\", \"features\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.08510638297872342\n",
      "Learned classification tree model:\n",
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4aa0a79c6d987d71bfeb) of depth 5 with 13 nodes\n",
      "  If (feature 2 <= 1.9)\n",
      "   Predict: 0.0\n",
      "  Else (feature 2 > 1.9)\n",
      "   If (feature 2 <= 4.7)\n",
      "    If (feature 3 <= 1.6)\n",
      "     Predict: 2.0\n",
      "    Else (feature 3 > 1.6)\n",
      "     Predict: 1.0\n",
      "   Else (feature 2 > 4.7)\n",
      "    If (feature 2 <= 5.0)\n",
      "     If (feature 1 <= 2.8)\n",
      "      Predict: 1.0\n",
      "     Else (feature 1 > 2.8)\n",
      "      If (feature 0 <= 5.9)\n",
      "       Predict: 2.0\n",
      "      Else (feature 0 > 5.9)\n",
      "       Predict: 1.0\n",
      "    Else (feature 2 > 5.0)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#评估决策树分类模型\n",
    "evaluatorClassifier = MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n",
    " \n",
    "accuracy = evaluatorClassifier.evaluate(predictionsClassifier)\n",
    " \n",
    "print(\"Test Error = \" + str(1.0 - accuracy))\n",
    "#Test Error = 0.05882352941176472\n",
    " \n",
    "treeModelClassifier = modelClassifier.stages[2]\n",
    " \n",
    "print(\"Learned classification tree model:\\n\" + str(treeModelClassifier.toDebugString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-----------------+\n",
      "|predictedLabel|label|         features|\n",
      "+--------------+-----+-----------------+\n",
      "|             0|    0|[4.4,2.9,1.4,0.2]|\n",
      "|             0|    0|[4.4,3.0,1.3,0.2]|\n",
      "|             0|    0|[4.7,3.2,1.6,0.2]|\n",
      "|             0|    0|[4.8,3.4,1.9,0.2]|\n",
      "|             0|    0|[5.0,3.0,1.6,0.2]|\n",
      "|             0|    0|[5.0,3.2,1.2,0.2]|\n",
      "|             0|    0|[5.0,3.3,1.4,0.2]|\n",
      "|             0|    0|[5.0,3.4,1.6,0.4]|\n",
      "|             0|    0|[5.0,3.5,1.3,0.3]|\n",
      "|             1|    1|[5.1,2.5,3.0,1.1]|\n",
      "|             0|    0|[5.1,3.4,1.5,0.2]|\n",
      "|             0|    0|[5.1,3.5,1.4,0.3]|\n",
      "|             0|    0|[5.1,3.7,1.5,0.4]|\n",
      "|             0|    0|[5.1,3.8,1.5,0.3]|\n",
      "|             0|    0|[5.2,3.4,1.4,0.2]|\n",
      "|             0|    0|[5.4,3.4,1.7,0.2]|\n",
      "|             1|    1|[5.5,2.5,4.0,1.3]|\n",
      "|             2|    2|[5.6,2.8,4.9,2.0]|\n",
      "|             0|    0|[5.7,3.8,1.7,0.3]|\n",
      "|             1|    1|[5.8,2.6,4.0,1.2]|\n",
      "+--------------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#构建决策树回归模型\n",
    "#导入所需要的包\n",
    "from pyspark.ml.regression import DecisionTreeRegressionModel,DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "#训练决策树模型\n",
    "dtRegressor = DecisionTreeRegressor().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\")\n",
    "#在pipeline中进行设置\n",
    "pipelineRegressor = Pipeline().setStages([labelIndexer, featureIndexer, dtRegressor, labelConverter])\n",
    "#训练决策树模型\n",
    "modelRegressor = pipelineRegressor.fit(trainingData)\n",
    "#进行预测\n",
    "predictionsRegressor = modelRegressor.transform(testData)\n",
    "#查看部分预测结果\n",
    "predictionsRegressor.select(\"predictedLabel\", \"label\", \"features\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.27288841145490766\n",
      "Learned regression tree model:\n",
      "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_4e5f821ce44f8a1f0818) of depth 5 with 13 nodes\n",
      "  If (feature 2 <= 1.9)\n",
      "   Predict: 0.0\n",
      "  Else (feature 2 > 1.9)\n",
      "   If (feature 2 <= 4.7)\n",
      "    If (feature 3 <= 1.6)\n",
      "     Predict: 2.0\n",
      "    Else (feature 3 > 1.6)\n",
      "     Predict: 1.0\n",
      "   Else (feature 2 > 4.7)\n",
      "    If (feature 2 <= 5.0)\n",
      "     If (feature 1 <= 2.8)\n",
      "      Predict: 1.0\n",
      "     Else (feature 1 > 2.8)\n",
      "      If (feature 0 <= 5.9)\n",
      "       Predict: 2.0\n",
      "      Else (feature 0 > 5.9)\n",
      "       Predict: 1.5\n",
      "    Else (feature 2 > 5.0)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluatorRegressor = RegressionEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"rmse\")\n",
    " \n",
    "rmse = evaluatorRegressor.evaluate(predictionsRegressor)\n",
    " \n",
    "print(\"Root Mean Squared Error (RMSE) on test data = \" +str(rmse))\n",
    "#Root Mean Squared Error (RMSE) on test data = 0.24253562503633297\n",
    " \n",
    "treeModelRegressor = modelRegressor.stages[2]\n",
    " \n",
    "print(\"Learned regression tree model:\\n\" + str(treeModelRegressor.toDebugString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
