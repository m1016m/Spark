{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讓我們在交互式會話中讀取一個文件。我們將在這裡從spark文件夾中讀取“CHANGES.txt”文件\n",
    "RDDread = sc.textFile (\"file:///home/hadoop/CHANGES.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark Change Log',\n",
       " '----------------',\n",
       " '',\n",
       " 'Release 1.1.0',\n",
       " '',\n",
       " '  [SPARK-3320][SQL] Made batched in-memory column buffer building work for SchemaRDDs with empty partitions',\n",
       " '  Cheng Lian <lian.cs.zju@gmail.com>',\n",
       " '  2014-08-29 18:16:47 -0700',\n",
       " '  Commit: aa9364a, github.com/apache/spark/pull/2213',\n",
       " '',\n",
       " '  [SPARK-3296][mllib] spark-example should be run-example in head notation of DenseKMeans and SparseNaiveBayes',\n",
       " '  wangfei <wangfei_hello@126.com>',\n",
       " '  2014-08-29 17:37:15 -0700',\n",
       " '  Commit: b0facb5, github.com/apache/spark/pull/2193',\n",
       " '',\n",
       " '  [SPARK-3291][SQL]TestcaseName in createQueryTest should not contain \":\"',\n",
       " '  qiping.lqp <qiping.lqp@alibaba-inc.com>',\n",
       " '  2014-08-29 15:37:43 -0700',\n",
       " '  Commit: c1333b8, github.com/apache/spark/pull/2191',\n",
       " '',\n",
       " '  [SPARK-3269][SQL] Decreases initial buffer size for row set to prevent OOM',\n",
       " '  Cheng Lian <lian.cs.zju@gmail.com>',\n",
       " '  2014-08-29 15:36:04 -0700',\n",
       " '  Commit: 9bae345, github.com/apache/spark/pull/2171',\n",
       " '',\n",
       " '  [SPARK-3234][Build] Fixed environment variables that rely on deprecated command line options in make-distribution.sh',\n",
       " '  Cheng Lian <lian.cs.zju@gmail.com>',\n",
       " '  2014-08-29 15:29:43 -0700',\n",
       " '  Commit: cf049ef, github.com/apache/spark/pull/2208',\n",
       " '',\n",
       " '  [Docs] SQL doc formatting and typo fixes',\n",
       " '  Nicholas Chammas <nicholas.chammas@gmail.com>, nchammas <nicholas.chammas@gmail.com>',\n",
       " '  2014-08-29 15:23:32 -0700',\n",
       " '  Commit: bfa2dc9, github.com/apache/spark/pull/2201',\n",
       " '',\n",
       " '  [SPARK-3307] [PySpark] Fix doc string of SparkContext.broadcast()',\n",
       " '  Davies Liu <davies.liu@gmail.com>',\n",
       " '  2014-08-29 11:47:49 -0700',\n",
       " '  Commit: 98d0716, github.com/apache/spark/pull/2202',\n",
       " '',\n",
       " '  HOTFIX: Bump spark-ec2 version to 1.1.0',\n",
       " '  Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-29 11:20:45 -0700',\n",
       " '  Commit: c71b5c6',\n",
       " '',\n",
       " '  Adding new CHANGES.txt',\n",
       " '  Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-28 17:17:30 -0700',\n",
       " '  Commit: 7db87b3',\n",
       " '',\n",
       " '  [SPARK-3277] Fix external spilling with LZ4 assertion error',\n",
       " '  Andrew Or <andrewor14@gmail.com>, Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-28 17:05:21 -0700',\n",
       " '  Commit: fe4df34, github.com/apache/spark/pull/2187',\n",
       " '',\n",
       " '  SPARK-3082. yarn.Client.logClusterResourceDetails throws NPE if requeste...',\n",
       " '  Sandy Ryza <sandy@cloudera.com>',\n",
       " '  2014-08-28 16:18:50 -0700',\n",
       " '  Commit: f4cbf5e, github.com/apache/spark/pull/1984',\n",
       " '',\n",
       " '  [SPARK-3190] Avoid overflow in VertexRDD.count()',\n",
       " '  Ankur Dave <ankurdave@gmail.com>',\n",
       " '  2014-08-28 15:17:01 -0700',\n",
       " '  Commit: 0b9718a, github.com/apache/spark/pull/2106',\n",
       " '',\n",
       " '  [SPARK-3264] Allow users to set executor Spark home in Mesos',\n",
       " '  Andrew Or <andrewor14@gmail.com>',\n",
       " '  2014-08-28 11:05:44 -0700',\n",
       " '  Commit: 069ecfe, github.com/apache/spark/pull/2166',\n",
       " '',\n",
       " '  [SPARK-3150] Fix NullPointerException in in Spark recovery: Add initializing default values in DriverInfo.init()',\n",
       " '  Tatiana Borisova <tanyatik@yandex.ru>',\n",
       " '  2014-08-28 10:36:36 -0700',\n",
       " '  Commit: fd98020, github.com/apache/spark/pull/2062',\n",
       " '',\n",
       " '  Additional CHANGES.txt',\n",
       " '  Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-28 00:19:03 -0700',\n",
       " '  Commit: a9df703',\n",
       " '',\n",
       " '  [SPARK-3230][SQL] Fix udfs that return structs',\n",
       " '  Michael Armbrust <michael@databricks.com>',\n",
       " '  2014-08-28 00:15:23 -0700',\n",
       " '  Commit: 2e8ad99, github.com/apache/spark/pull/2133',\n",
       " '',\n",
       " '  [SQL] Fixed 2 comment typos in SQLConf',\n",
       " '  Cheng Lian <lian.cs.zju@gmail.com>',\n",
       " '  2014-08-28 00:08:09 -0700',\n",
       " '  Commit: c0e3bc1, github.com/apache/spark/pull/2172',\n",
       " '',\n",
       " \"  HOTFIX: Don't build with YARN support for Mapr3\",\n",
       " '  Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-27 15:40:40 -0700',\n",
       " '  Commit: ad0fab2',\n",
       " '',\n",
       " '  [HOTFIX][SQL] Remove cleaning of UDFs',\n",
       " '  Michael Armbrust <michael@databricks.com>',\n",
       " '  2014-08-27 23:05:34 -0700',\n",
       " '  Commit: 233c283, github.com/apache/spark/pull/2174',\n",
       " '',\n",
       " '  [HOTFIX] Wait for EOF only for the PySpark shell',\n",
       " '  Andrew Or <andrewor14@gmail.com>',\n",
       " '  2014-08-27 23:03:46 -0700',\n",
       " '  Commit: 54ccd93, github.com/apache/spark/pull/2170',\n",
       " '',\n",
       " '  BUILD: Updating CHANGES.txt for Spark 1.1',\n",
       " '  Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-27 15:55:59 -0700',\n",
       " '  Commit: 8597e9c',\n",
       " '',\n",
       " '  Add line continuation for script to work w/ py2.7.5',\n",
       " '  Matthew Farrellee <matt@redhat.com>',\n",
       " '  2014-08-27 15:50:30 -0700',\n",
       " '  Commit: d4cf7a0, github.com/apache/spark/pull/2139',\n",
       " '',\n",
       " \"  [SPARK-3235][SQL] Ensure in-memory tables don't always broadcast.\",\n",
       " '  Michael Armbrust <michael@databricks.com>',\n",
       " '  2014-08-27 15:14:08 -0700',\n",
       " '  Commit: 9a62cf3, github.com/apache/spark/pull/2147',\n",
       " '',\n",
       " '  [SPARK-3065][SQL] Add locale setting to fix results do not match for udf_unix_timestamp format \"yyyy MMM dd h:mm:ss a\" run with not \"America/Los_Angeles\" TimeZone in HiveCompatibilitySuite',\n",
       " '  luogankun <luogankun@gmail.com>',\n",
       " '  2014-08-27 15:08:22 -0700',\n",
       " '  Commit: 5ea260e, github.com/apache/spark/pull/1968',\n",
       " '',\n",
       " '  [SQL] [SPARK-3236] Reading Parquet tables from Metastore mangles location',\n",
       " '  Aaron Davidson <aaron@databricks.com>',\n",
       " '  2014-08-27 15:05:47 -0700',\n",
       " '  Commit: 7711687, github.com/apache/spark/pull/2150',\n",
       " '',\n",
       " '  [SPARK-3252][SQL] Add missing condition for test',\n",
       " '  viirya <viirya@gmail.com>',\n",
       " '  2014-08-27 14:55:05 -0700',\n",
       " '  Commit: b3d763b, github.com/apache/spark/pull/2159',\n",
       " '',\n",
       " \"  [SPARK-3243] Don't use stale spark-driver.* system properties\",\n",
       " '  Andrew Or <andrewor14@gmail.com>',\n",
       " '  2014-08-27 14:46:56 -0700',\n",
       " '  Commit: c1ffa3e, github.com/apache/spark/pull/2154',\n",
       " '',\n",
       " '  Spark-3213 Fixes issue with spark-ec2 not detecting slaves created with \"Launch More like this\"',\n",
       " '  Vida Ha <vida@databricks.com>',\n",
       " '  2014-08-27 14:26:06 -0700',\n",
       " '  Commit: 3cb4e17, github.com/apache/spark/pull/2163',\n",
       " '',\n",
       " '  [SPARK-3138][SQL] sqlContext.parquetFile should be able to take a single file as parameter',\n",
       " '  chutium <teng.qiu@gmail.com>',\n",
       " '  2014-08-27 13:13:04 -0700',\n",
       " '  Commit: 90f8f3e, github.com/apache/spark/pull/2044',\n",
       " '',\n",
       " '  [SPARK-3197] [SQL] Reduce the Expression tree object creations for aggregation function (min/max)',\n",
       " '  Cheng Hao <hao.cheng@intel.com>',\n",
       " '  2014-08-27 12:50:47 -0700',\n",
       " '  Commit: 4c7f082, github.com/apache/spark/pull/2113',\n",
       " '',\n",
       " '  [SPARK-3118][SQL]add \"SHOW TBLPROPERTIES tblname;\" and \"SHOW COLUMNS (FROM|IN) table_name [(FROM|IN) db_name]\" support',\n",
       " '  u0jing <u9jing@gmail.com>',\n",
       " '  2014-08-27 12:47:14 -0700',\n",
       " '  Commit: 19cda07, github.com/apache/spark/pull/2034',\n",
       " '',\n",
       " '  SPARK-3259 - User data should be given to the master',\n",
       " '  Allan Douglas R. de Oliveira <allan@chaordicsystems.com>',\n",
       " '  2014-08-27 12:43:22 -0700',\n",
       " '  Commit: 0c94a5b, github.com/apache/spark/pull/2162',\n",
       " '',\n",
       " '  [SPARK-2608][Core] Fixed command line option passing issue over Mesos via SPARK_EXECUTOR_OPTS',\n",
       " '  Cheng Lian <lian.cs.zju@gmail.com>',\n",
       " '  2014-08-27 12:39:21 -0700',\n",
       " '  Commit: 935bffe, github.com/apache/spark/pull/2161',\n",
       " '',\n",
       " '  [SPARK-3239] [PySpark] randomize the dirs for each process',\n",
       " '  Davies Liu <davies.liu@gmail.com>',\n",
       " '  2014-08-27 10:40:35 -0700',\n",
       " '  Commit: 092121e, github.com/apache/spark/pull/2152',\n",
       " '',\n",
       " '  [SPARK-3170][CORE][BUG]:RDD info loss in \"StorageTab\" and \"ExecutorTab\"',\n",
       " '  uncleGen <hustyugm@gmail.com>',\n",
       " '  2014-08-27 10:32:13 -0700',\n",
       " '  Commit: 8f8e2a4, github.com/apache/spark/pull/2131',\n",
       " '',\n",
       " '  [SPARK-3154][STREAMING] Make FlumePollingInputDStream shutdown cleaner.',\n",
       " '  Hari Shreedharan <hshreedharan@apache.org>',\n",
       " '  2014-08-27 02:39:02 -0700',\n",
       " '  Commit: 1d468df, github.com/apache/spark/pull/2065',\n",
       " '',\n",
       " '  [SPARK-3227] [mllib] Added migration guide for v1.0 to v1.1',\n",
       " '  Joseph K. Bradley <joseph.kurata.bradley@gmail.com>',\n",
       " '  2014-08-27 01:45:59 -0700',\n",
       " '  Commit: 7286d57, github.com/apache/spark/pull/2146',\n",
       " '',\n",
       " '  [SPARK-2830][MLLIB] doc update for 1.1',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-27 01:19:48 -0700',\n",
       " '  Commit: 7401247, github.com/apache/spark/pull/2151',\n",
       " '',\n",
       " '  [SPARK-3237][SQL] Fix parquet filters with UDFs',\n",
       " '  Michael Armbrust <michael@databricks.com>',\n",
       " '  2014-08-27 00:59:23 -0700',\n",
       " '  Commit: ca01de1, github.com/apache/spark/pull/2153',\n",
       " '',\n",
       " '  [SPARK-3139] Made ContextCleaner to not block on shuffles',\n",
       " '  Tathagata Das <tathagata.das1565@gmail.com>',\n",
       " '  2014-08-27 00:13:38 -0700',\n",
       " '  Commit: 5cf1e44, github.com/apache/spark/pull/2143',\n",
       " '',\n",
       " '  HOTFIX: Minor typo in conf template',\n",
       " '  Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-26 23:40:50 -0700',\n",
       " '  Commit: 6f82a4b',\n",
       " '',\n",
       " '  [SPARK-3167] Handle special driver configs in Windows (Branch 1.1)',\n",
       " '  Andrew Or <andrewor14@gmail.com>',\n",
       " '  2014-08-26 23:06:11 -0700',\n",
       " '  Commit: e7672f1, github.com/apache/spark/pull/2156',\n",
       " '',\n",
       " '  [SPARK-3224] FetchFailed reduce stages should only show up once in failed stages (in UI)',\n",
       " '  Reynold Xin <rxin@apache.org>, Kay Ousterhout <kayousterhout@gmail.com>',\n",
       " '  2014-08-26 21:59:48 -0700',\n",
       " '  Commit: 2381e90, github.com/apache/spark/pull/2127',\n",
       " '',\n",
       " '  Fix unclosed HTML tag in Yarn docs.',\n",
       " '  Josh Rosen <joshrosen@apache.org>',\n",
       " '  2014-08-26 18:55:00 -0700',\n",
       " '  Commit: 7726e56',\n",
       " '',\n",
       " '  [SPARK-3036][SPARK-3037][SQL] Add MapType/ArrayType containing null value support to Parquet.',\n",
       " '  Takuya UESHIN <ueshin@happy-camper.st>',\n",
       " '  2014-08-26 18:28:41 -0700',\n",
       " '  Commit: 8b5af6f, github.com/apache/spark/pull/2032',\n",
       " '',\n",
       " '  [Docs] Run tests like in contributing guide',\n",
       " '  nchammas <nicholas.chammas@gmail.com>',\n",
       " '  2014-08-26 17:50:04 -0700',\n",
       " '  Commit: 0d97233, github.com/apache/spark/pull/2149',\n",
       " '',\n",
       " '  [SPARK-2964] [SQL] Remove duplicated code from spark-sql and start-thriftserver.sh',\n",
       " '  Cheng Lian <lian.cs.zju@gmail.com>, Kousuke Saruta <sarutak@oss.nttdata.co.jp>',\n",
       " '  2014-08-26 17:33:40 -0700',\n",
       " '  Commit: c0e1f99, github.com/apache/spark/pull/1886',\n",
       " '',\n",
       " '  [SPARK-3194][SQL] Add AttributeSet to fix bugs with invalid comparisons of AttributeReferences',\n",
       " '  Michael Armbrust <michael@databricks.com>',\n",
       " '  2014-08-26 16:29:14 -0700',\n",
       " '  Commit: a308a16, github.com/apache/spark/pull/2109',\n",
       " '',\n",
       " '  [SPARK-2839][MLlib] Stats Toolkit documentation updated',\n",
       " '  Burak <brkyvz@gmail.com>',\n",
       " '  2014-08-26 15:18:42 -0700',\n",
       " '  Commit: 2715eb7, github.com/apache/spark/pull/2130',\n",
       " '',\n",
       " '  [SPARK-3226][MLLIB] doc update for native libraries',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-26 15:12:27 -0700',\n",
       " '  Commit: 5ff9000, github.com/apache/spark/pull/2128',\n",
       " '',\n",
       " '  [SPARK-3063][SQL] ExistingRdd should convert Map to catalyst Map.',\n",
       " '  Takuya UESHIN <ueshin@happy-camper.st>',\n",
       " '  2014-08-26 15:04:08 -0700',\n",
       " '  Commit: 5d981a4, github.com/apache/spark/pull/1963',\n",
       " '',\n",
       " '  [SPARK-2969][SQL] Make ScalaReflection be able to handle ArrayType.containsNull and MapType.valueContainsNull.',\n",
       " '  Takuya UESHIN <ueshin@happy-camper.st>',\n",
       " '  2014-08-26 13:22:55 -0700',\n",
       " '  Commit: 35a5853, github.com/apache/spark/pull/1889',\n",
       " '',\n",
       " '  [SPARK-2871] [PySpark] add histgram() API',\n",
       " '  Davies Liu <davies.liu@gmail.com>',\n",
       " '  2014-08-26 13:04:30 -0700',\n",
       " '  Commit: 83d2730, github.com/apache/spark/pull/2091',\n",
       " '',\n",
       " '  [SPARK-3131][SQL] Allow user to set parquet compression codec for writing ParquetFile in SQLContext',\n",
       " '  chutium <teng.qiu@gmail.com>',\n",
       " '  2014-08-26 11:51:26 -0700',\n",
       " '  Commit: 3a9d874, github.com/apache/spark/pull/2039',\n",
       " '',\n",
       " '  [SPARK-2886] Use more specific actor system name than \"spark\"',\n",
       " '  Andrew Or <andrewor14@gmail.com>',\n",
       " '  2014-08-25 23:36:09 -0700',\n",
       " '  Commit: 0f947f1, github.com/apache/spark/pull/1810',\n",
       " '',\n",
       " '  [Spark-3222] [SQL] Cross join support in HiveQL',\n",
       " '  Daoyuan Wang <daoyuan.wang@intel.com>, adrian-wang <daoyuanwong@gmail.com>',\n",
       " '  2014-08-25 22:56:35 -0700',\n",
       " '  Commit: 48a0749, github.com/apache/spark/pull/2124',\n",
       " '',\n",
       " '  SPARK-2481: The environment variables SPARK_HISTORY_OPTS is covered in spark-env.sh',\n",
       " '  witgo <witgo@qq.com>, GuoQiang Li <witgo@qq.com>',\n",
       " '  2014-08-25 19:22:27 -0700',\n",
       " '  Commit: 4d6a0e9, github.com/apache/spark/pull/1341',\n",
       " '',\n",
       " '  [SPARK-3011][SQL] _temporary directory should be filtered out by sqlContext.parquetFile',\n",
       " '  Chia-Yung Su <chiayung@appier.com>',\n",
       " '  2014-08-25 18:20:19 -0700',\n",
       " '  Commit: b5dc9b4, github.com/apache/spark/pull/1959',\n",
       " '',\n",
       " '  [SQL] logWarning should be logInfo in getResultSetSchema',\n",
       " '  wangfei <wangfei_hello@126.com>',\n",
       " '  2014-08-25 17:46:43 -0700',\n",
       " '  Commit: 957b356, github.com/apache/spark/pull/1939',\n",
       " '',\n",
       " '  [SPARK-3058] [SQL] Support EXTENDED for EXPLAIN',\n",
       " '  Cheng Hao <hao.cheng@intel.com>',\n",
       " '  2014-08-25 17:43:56 -0700',\n",
       " '  Commit: f8ac8ed, github.com/apache/spark/pull/1962',\n",
       " '',\n",
       " '  [SPARK-2929][SQL] Refactored Thrift server and CLI suites',\n",
       " '  Cheng Lian <lian.cs.zju@gmail.com>',\n",
       " '  2014-08-25 16:29:59 -0700',\n",
       " '  Commit: 292f28d, github.com/apache/spark/pull/1856',\n",
       " '',\n",
       " '  [SPARK-3204][SQL] MaxOf would be foldable if both left and right are foldable.',\n",
       " '  Takuya UESHIN <ueshin@happy-camper.st>',\n",
       " '  2014-08-25 16:27:00 -0700',\n",
       " '  Commit: 19b01d6, github.com/apache/spark/pull/2116',\n",
       " '',\n",
       " '  Fixed a typo in docs/running-on-mesos.md',\n",
       " '  Cheng Lian <lian.cs.zju@gmail.com>',\n",
       " '  2014-08-25 14:56:51 -0700',\n",
       " '  Commit: 8d33a6d, github.com/apache/spark/pull/2119',\n",
       " '',\n",
       " '  [FIX] fix error message in sendMessageReliably',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-25 14:55:20 -0700',\n",
       " '  Commit: d892062, github.com/apache/spark/pull/2120',\n",
       " '',\n",
       " '  SPARK-2798 [BUILD] Correct several small errors in Flume module pom.xml files',\n",
       " '  Sean Owen <sowen@cloudera.com>',\n",
       " '  2014-08-25 13:29:07 -0700',\n",
       " '  Commit: ff616fd, github.com/apache/spark/pull/1726',\n",
       " '',\n",
       " '  [SPARK-2495][MLLIB] make KMeans constructor public',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-25 12:30:02 -0700',\n",
       " '  Commit: 69a17f1, github.com/apache/spark/pull/2112',\n",
       " '',\n",
       " '  [SPARK-2871] [PySpark] add zipWithIndex() and zipWithUniqueId()',\n",
       " '  Davies Liu <davies.liu@gmail.com>',\n",
       " '  2014-08-24 21:16:05 -0700',\n",
       " '  Commit: b82da3d, github.com/apache/spark/pull/2092',\n",
       " '',\n",
       " '  [MLlib][SPARK-2997] Update SVD documentation to reflect roughly square',\n",
       " '  Reza Zadeh <rizlar@gmail.com>',\n",
       " '  2014-08-24 17:35:54 -0700',\n",
       " '  Commit: 749bddc, github.com/apache/spark/pull/2070',\n",
       " '',\n",
       " '  [SPARK-2841][MLlib] Documentation for feature transformations',\n",
       " '  DB Tsai <dbtsai@alpinenow.com>',\n",
       " '  2014-08-24 17:33:33 -0700',\n",
       " '  Commit: a4db81a, github.com/apache/spark/pull/2068',\n",
       " '',\n",
       " '  [SPARK-3192] Some scripts have 2 space indentation but other scripts have 4 space indentation.',\n",
       " '  Kousuke Saruta <sarutak@oss.nttdata.co.jp>',\n",
       " '  2014-08-24 09:43:44 -0700',\n",
       " '  Commit: ce14cd1, github.com/apache/spark/pull/2104',\n",
       " '',\n",
       " '  [SPARK-2967][SQL]  Follow-up: Also copy hash expressions in sort based shuffle fix.',\n",
       " '  Michael Armbrust <michael@databricks.com>',\n",
       " '  2014-08-23 16:21:08 -0700',\n",
       " '  Commit: e23f0bc, github.com/apache/spark/pull/2072',\n",
       " '',\n",
       " '  [SPARK-2554][SQL] CountDistinct partial aggregation and object allocation improvements',\n",
       " '  Michael Armbrust <michael@databricks.com>, Gregory Owen <greowen@gmail.com>',\n",
       " '  2014-08-23 16:19:10 -0700',\n",
       " '  Commit: 7112da8, github.com/apache/spark/pull/1935',\n",
       " '',\n",
       " '  [SQL] Make functionRegistry in HiveContext transient.',\n",
       " '  Yin Huai <huaiyin.thu@gmail.com>',\n",
       " '  2014-08-23 12:46:41 -0700',\n",
       " '  Commit: 9309786, github.com/apache/spark/pull/2074',\n",
       " '',\n",
       " '  [SPARK-2963] REGRESSION - The description about how to build for using CLI and Thrift JDBC server is absent in proper document  -',\n",
       " '  Kousuke Saruta <sarutak@oss.nttdata.co.jp>',\n",
       " '  2014-08-22 22:28:05 -0700',\n",
       " '  Commit: 5689660, github.com/apache/spark/pull/2080',\n",
       " '',\n",
       " '  [SPARK-3169] Removed dependency on spark streaming test from spark flume sink',\n",
       " '  Tathagata Das <tathagata.das1565@gmail.com>',\n",
       " '  2014-08-22 21:34:48 -0700',\n",
       " '  Commit: cd73631, github.com/apache/spark/pull/2101',\n",
       " '',\n",
       " '  Revert \"HOTFIX:Temporarily removing flume sink test in 1.1 branch\"',\n",
       " '  Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-22 21:31:52 -0700',\n",
       " '  Commit: 385c4f2',\n",
       " '',\n",
       " '  [SPARK-2840] [mllib] DecisionTree doc update (Java, Python examples)',\n",
       " '  Joseph K. Bradley <joseph.kurata.bradley@gmail.com>',\n",
       " '  2014-08-21 00:17:29 -0700',\n",
       " '  Commit: 1e5d9cb, github.com/apache/spark/pull/2063',\n",
       " '',\n",
       " '  BUILD: Bump Hadoop versions in the release build.',\n",
       " '  Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-20 12:18:41 -0700',\n",
       " '  Commit: da0a701',\n",
       " '',\n",
       " '  HOTFIX:Temporarily removing flume sink test in 1.1 branch',\n",
       " '  Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-20 22:24:22 -0700',\n",
       " '  Commit: 1d5e84a',\n",
       " '',\n",
       " '  [HOTFIX][STREAMING] Allow the JVM/Netty to decide which port to bind to in Flume Polling Tests.',\n",
       " '  Hari Shreedharan <harishreedharan@gmail.com>',\n",
       " '  2014-08-17 19:50:31 -0700',\n",
       " '  Commit: 4485665, github.com/apache/spark/pull/1820',\n",
       " '',\n",
       " '  [HOTFIX][Streaming] Handle port collisions in flume polling test',\n",
       " '  Andrew Or <andrewor14@gmail.com>',\n",
       " '  2014-08-06 16:34:53 -0700',\n",
       " '  Commit: 3f91e9d, github.com/apache/spark/pull/1803',\n",
       " '',\n",
       " '  [SPARK-2843][MLLIB] add a section about regularization parameter in ALS',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-20 17:47:39 -0700',\n",
       " '  Commit: eba399b, github.com/apache/spark/pull/2064',\n",
       " '',\n",
       " '  [SPARK-3143][MLLIB] add tf-idf user guide',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-20 17:41:36 -0700',\n",
       " '  Commit: 1af68ca, github.com/apache/spark/pull/2061',\n",
       " '',\n",
       " '  [SPARK-3140] Clarify confusing PySpark exception message',\n",
       " '  Andrew Or <andrewor14@gmail.com>',\n",
       " '  2014-08-20 17:07:39 -0700',\n",
       " '  Commit: f8bcb12, github.com/apache/spark/pull/2067',\n",
       " '',\n",
       " '  [SPARK-2298] Encode stage attempt in SparkListener & UI.',\n",
       " '  Reynold Xin <rxin@apache.org>',\n",
       " '  2014-08-20 15:37:27 -0700',\n",
       " '  Commit: dc05282, github.com/apache/spark/pull/1545',\n",
       " '',\n",
       " \"  [SPARK-2169] Don't copy appName / basePath everywhere.\",\n",
       " '  Marcelo Vanzin <vanzin@cloudera.com>',\n",
       " '  2014-08-18 13:25:30 -0700',\n",
       " '  Commit: 2c1683e, github.com/apache/spark/pull/1252',\n",
       " '',\n",
       " '  [SPARK-2846][SQL] Add configureInputJobPropertiesForStorageHandler to initialization of job conf',\n",
       " '  Alex Liu <alex_liu68@yahoo.com>',\n",
       " '  2014-08-20 16:14:06 -0700',\n",
       " '  Commit: 64e136a, github.com/apache/spark/pull/1927',\n",
       " '',\n",
       " '  SPARK_LOGFILE and SPARK_ROOT_LOGGER no longer need in spark-daemon.sh',\n",
       " '  wangfei <wangfei_hello@126.com>',\n",
       " '  2014-08-20 16:00:46 -0700',\n",
       " '  Commit: 5f72d7b, github.com/apache/spark/pull/2057',\n",
       " '',\n",
       " '  [SPARK-2967][SQL] Fix sort based shuffle for spark sql.',\n",
       " '  Michael Armbrust <michael@databricks.com>',\n",
       " '  2014-08-20 15:51:14 -0700',\n",
       " '  Commit: 311831d, github.com/apache/spark/pull/2066',\n",
       " '',\n",
       " '  [SPARK-2849] Handle driver configs separately in client mode',\n",
       " '  Andrew Or <andrewor14@gmail.com>',\n",
       " '  2014-08-20 15:01:47 -0700',\n",
       " '  Commit: beb705a, github.com/apache/spark/pull/1845',\n",
       " '',\n",
       " '  [SPARK-3149] Connection establishment information is not enough.',\n",
       " '  Kousuke Saruta <sarutak@oss.nttdata.co.jp>',\n",
       " '  2014-08-20 14:04:39 -0700',\n",
       " '  Commit: 25b01fd, github.com/apache/spark/pull/2060',\n",
       " '',\n",
       " '  [SPARK-3062] [SPARK-2970] [SQL] spark-sql script ends with IOException when EventLogging is enabled',\n",
       " '  Kousuke Saruta <sarutak@oss.nttdata.co.jp>',\n",
       " '  2014-08-20 13:26:11 -0700',\n",
       " '  Commit: 5095851, github.com/apache/spark/pull/1970',\n",
       " '',\n",
       " '  [SPARK-3126][SPARK-3127][SQL] Fixed HiveThriftServer2Suite',\n",
       " '  Cheng Lian <lian.cs.zju@gmail.com>',\n",
       " '  2014-08-20 12:57:39 -0700',\n",
       " '  Commit: 99ca704, github.com/apache/spark/pull/2036',\n",
       " '',\n",
       " '  SPARK-3092 [SQL]: Always include the thriftserver when -Phive is enabled.',\n",
       " '  Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-20 12:13:31 -0700',\n",
       " '  Commit: ca7322d, github.com/apache/spark/pull/2006',\n",
       " '',\n",
       " '  [SPARK-3054][STREAMING] Add unit tests for Spark Sink.',\n",
       " '  Hari Shreedharan <hshreedharan@apache.org>, Hari Shreedharan <hshreedharan@cloudera.com>',\n",
       " '  2014-08-20 04:09:54 -0700',\n",
       " '  Commit: 9b29099, github.com/apache/spark/pull/1958',\n",
       " '',\n",
       " '  [SPARK-3141] [PySpark] fix sortByKey() with take()',\n",
       " '  Davies Liu <davies.liu@gmail.com>',\n",
       " '  2014-08-19 22:43:49 -0700',\n",
       " '  Commit: 5b22ebf, github.com/apache/spark/pull/2045',\n",
       " '',\n",
       " '  [DOCS] Fixed wrong links',\n",
       " '  Ken Takagiwa <ugw.gi.world@gmail.com>',\n",
       " '  2014-08-19 22:43:22 -0700',\n",
       " '  Commit: f8c908e, github.com/apache/spark/pull/2042',\n",
       " '',\n",
       " '  [SPARK-2974] [SPARK-2975] Fix two bugs related to spark.local.dirs',\n",
       " '  Josh Rosen <joshrosen@apache.org>',\n",
       " '  2014-08-19 22:42:50 -0700',\n",
       " '  Commit: 5d1a878, github.com/apache/spark/pull/2002',\n",
       " '',\n",
       " '  [SPARK-3142][MLLIB] output shuffle data directly in Word2Vec',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-19 22:16:22 -0700',\n",
       " '  Commit: a5bc9c6, github.com/apache/spark/pull/2049',\n",
       " '',\n",
       " '  [SPARK-3119] Re-implementation of TorrentBroadcast.',\n",
       " '  Reynold Xin <rxin@apache.org>',\n",
       " '  2014-08-19 22:11:13 -0700',\n",
       " '  Commit: 08c9973, github.com/apache/spark/pull/2030',\n",
       " '',\n",
       " '  [HOTFIX][Streaming][MLlib] use temp folder for checkpoint',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-19 22:05:29 -0700',\n",
       " '  Commit: d5db95b, github.com/apache/spark/pull/2046',\n",
       " '',\n",
       " '  [SPARK-3130][MLLIB] detect negative values in naive Bayes',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-19 21:01:23 -0700',\n",
       " '  Commit: 148e45b, github.com/apache/spark/pull/2038',\n",
       " '',\n",
       " '  [SQL] add note of use synchronizedMap in SQLConf',\n",
       " '  wangfei <wangfei_hello@126.com>, scwf <wangfei1@huawei.com>',\n",
       " '  2014-08-19 19:37:02 -0700',\n",
       " '  Commit: 607735c, github.com/apache/spark/pull/1996',\n",
       " '',\n",
       " '  [SPARK-3112][MLLIB] Add documentation and example for StreamingLR',\n",
       " '  freeman <the.freeman.lab@gmail.com>',\n",
       " '  2014-08-19 18:07:42 -0700',\n",
       " '  Commit: d75464d, github.com/apache/spark/pull/2047',\n",
       " '',\n",
       " '  [MLLIB] minor update to word2vec',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-19 17:41:37 -0700',\n",
       " '  Commit: 023ed7c, github.com/apache/spark/pull/2043',\n",
       " '',\n",
       " '  [SPARK-2468] Netty based block server / client module',\n",
       " '  Reynold Xin <rxin@apache.org>',\n",
       " '  2014-08-19 17:40:35 -0700',\n",
       " '  Commit: 66b4c81, github.com/apache/spark/pull/1971',\n",
       " '',\n",
       " '  [SPARK-3136][MLLIB] Create Java-friendly methods in RandomRDDs',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-19 16:06:48 -0700',\n",
       " '  Commit: d371c71, github.com/apache/spark/pull/2041',\n",
       " '',\n",
       " '  [SPARK-2790] [PySpark] fix zip with serializers which have different batch sizes.',\n",
       " '  Davies Liu <davies.liu@gmail.com>',\n",
       " '  2014-08-19 14:46:32 -0700',\n",
       " '  Commit: 3540d4b, github.com/apache/spark/pull/1894',\n",
       " '',\n",
       " '  Move a bracket in validateSettings of SparkConf',\n",
       " '  hzw19900416 <carlmartinmax@gmail.com>',\n",
       " '  2014-08-19 14:04:49 -0700',\n",
       " '  Commit: f6b4ab8, github.com/apache/spark/pull/2012',\n",
       " '',\n",
       " '  SPARK-2333 - spark_ec2 script should allow option for existing security group',\n",
       " '  Vida Ha <vida@databricks.com>',\n",
       " '  2014-08-19 13:35:05 -0700',\n",
       " '  Commit: c3952b0, github.com/apache/spark/pull/1899',\n",
       " '',\n",
       " '  [SPARK-3128][MLLIB] Use streaming test suite for StreamingLR',\n",
       " '  freeman <the.freeman.lab@gmail.com>',\n",
       " '  2014-08-19 13:28:57 -0700',\n",
       " '  Commit: 04a3208, github.com/apache/spark/pull/2037',\n",
       " '',\n",
       " '  [SPARK-3089] Fix meaningless error message in ConnectionManager',\n",
       " '  Kousuke Saruta <sarutak@oss.nttdata.co.jp>',\n",
       " '  2014-08-19 10:15:11 -0700',\n",
       " '  Commit: 5d895ad, github.com/apache/spark/pull/2000',\n",
       " '',\n",
       " '  [SPARK-3072] YARN - Exit when reach max number failed executors',\n",
       " '  Thomas Graves <tgraves@apache.org>',\n",
       " '  2014-08-19 09:40:31 -0500',\n",
       " '  Commit: 1418893, github.com/apache/spark/pull/2022',\n",
       " '',\n",
       " '  Fix typo in decision tree docs',\n",
       " '  Matt Forbes <matt@tellapart.com>',\n",
       " '  2014-08-18 21:43:32 -0700',\n",
       " '  Commit: f3b0f34, github.com/apache/spark/pull/1837',\n",
       " '',\n",
       " '  [SPARK-3116] Remove the excessive lockings in TorrentBroadcast',\n",
       " '  Reynold Xin <rxin@apache.org>',\n",
       " '  2014-08-18 20:51:41 -0700',\n",
       " '  Commit: b6d8e66, github.com/apache/spark/pull/2028',\n",
       " '',\n",
       " '  [SPARK-3114] [PySpark] Fix Python UDFs in Spark SQL.',\n",
       " '  Josh Rosen <joshrosen@apache.org>, Davies Liu <davies.liu@gmail.com>',\n",
       " '  2014-08-18 20:42:19 -0700',\n",
       " '  Commit: 3a03259, github.com/apache/spark/pull/2026.',\n",
       " '',\n",
       " '  [SPARK-3108][MLLIB] add predictOnValues to StreamingLR and fix predictOn',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-18 18:20:54 -0700',\n",
       " '  Commit: 7d069bf, github.com/apache/spark/pull/2023',\n",
       " '',\n",
       " '  [SPARK-2850] [SPARK-2626] [mllib] MLlib stats examples + small fixes',\n",
       " '  Joseph K. Bradley <joseph.kurata.bradley@gmail.com>',\n",
       " '  2014-08-18 18:01:39 -0700',\n",
       " '  Commit: e3f89e9, github.com/apache/spark/pull/1878',\n",
       " '',\n",
       " '  [mllib] DecisionTree: treeAggregate + Python example bug fix',\n",
       " '  Joseph K. Bradley <joseph.kurata.bradley@gmail.com>',\n",
       " '  2014-08-18 14:40:05 -0700',\n",
       " '  Commit: 98778ff, github.com/apache/spark/pull/2015',\n",
       " '',\n",
       " '  [SPARK-2718] [yarn] Handle quotes and other characters in user args.',\n",
       " '  Marcelo Vanzin <vanzin@cloudera.com>',\n",
       " '  2014-08-18 14:10:10 -0700',\n",
       " '  Commit: 25cabd7, github.com/apache/spark/pull/1724',\n",
       " '',\n",
       " '  [SPARK-3103] [PySpark] fix saveAsTextFile() with utf-8',\n",
       " '  Davies Liu <davies.liu@gmail.com>',\n",
       " '  2014-08-18 13:58:35 -0700',\n",
       " '  Commit: e083334, github.com/apache/spark/pull/2018',\n",
       " '',\n",
       " '  [SPARK-2406][SQL] Initial support for using ParquetTableScan to read HiveMetaStore tables.',\n",
       " '  Michael Armbrust <michael@databricks.com>, Yin Huai <huai@cse.ohio-state.edu>',\n",
       " '  2014-08-18 13:17:10 -0700',\n",
       " '  Commit: cc4015d, github.com/apache/spark/pull/1819',\n",
       " '',\n",
       " '  [SPARK-3091] [SQL] Add support for caching metadata on Parquet files',\n",
       " '  Matei Zaharia <matei@databricks.com>',\n",
       " '  2014-08-18 11:00:10 -0700',\n",
       " '  Commit: 2ae2857, github.com/apache/spark/pull/2005',\n",
       " '',\n",
       " '  SPARK-3025 [SQL]: Allow JDBC clients to set a fair scheduler pool',\n",
       " '  Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-18 10:52:20 -0700',\n",
       " '  Commit: 496f62d, github.com/apache/spark/pull/1937',\n",
       " '',\n",
       " '  [SPARK-3085] [SQL] Use compact data structures in SQL joins',\n",
       " '  Matei Zaharia <matei@databricks.com>',\n",
       " '  2014-08-18 10:45:24 -0700',\n",
       " '  Commit: 4da76fc, github.com/apache/spark/pull/1993',\n",
       " '',\n",
       " '  [SPARK-3084] [SQL] Collect broadcasted tables in parallel in joins',\n",
       " '  Matei Zaharia <matei@databricks.com>',\n",
       " '  2014-08-18 10:05:52 -0700',\n",
       " '  Commit: 55e9dd6, github.com/apache/spark/pull/1990',\n",
       " '',\n",
       " '  SPARK-3096: Include parquet hive serde by default in build',\n",
       " '  Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-18 10:00:46 -0700',\n",
       " '  Commit: ec0b91e, github.com/apache/spark/pull/2009',\n",
       " '',\n",
       " '  [SPARK-2862] histogram method fails on some choices of bucketCount',\n",
       " '  Chandan Kumar <chandan.kumar@imaginea.com>',\n",
       " '  2014-08-18 09:52:25 -0700',\n",
       " '  Commit: 12f16ba, github.com/apache/spark/pull/1787',\n",
       " '',\n",
       " '  [MLlib] Remove transform(dataset: RDD[String]) from Word2Vec public API',\n",
       " '  Liquan Pei <liquanpei@gmail.com>',\n",
       " '  2014-08-18 01:15:45 -0700',\n",
       " '  Commit: e0bc333, github.com/apache/spark/pull/2010',\n",
       " '',\n",
       " '  [SPARK-2842][MLlib]Word2Vec documentation',\n",
       " '  Liquan Pei <liquanpei@gmail.com>',\n",
       " '  2014-08-17 23:30:47 -0700',\n",
       " '  Commit: 518258f, github.com/apache/spark/pull/2003',\n",
       " '',\n",
       " '  [SPARK-3097][MLlib] Word2Vec performance improvement',\n",
       " '  Liquan Pei <liquanpei@gmail.com>',\n",
       " '  2014-08-17 23:29:44 -0700',\n",
       " '  Commit: 708cde9, github.com/apache/spark/pull/1932',\n",
       " '',\n",
       " '  SPARK-2900. aggregate inputBytes per stage',\n",
       " '  Sandy Ryza <sandy@cloudera.com>',\n",
       " '  2014-08-17 22:39:06 -0700',\n",
       " '  Commit: 0506539, github.com/apache/spark/pull/1826',\n",
       " '',\n",
       " '  SPARK-2884: Create binary builds in parallel with release script.',\n",
       " '  Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-17 22:29:58 -0700',\n",
       " '  Commit: a5ae720',\n",
       " '',\n",
       " '  [SPARK-3087][MLLIB] fix col indexing bug in chi-square and add a check for number of distinct values',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-17 20:53:18 -0700',\n",
       " '  Commit: 8438daf, github.com/apache/spark/pull/1997',\n",
       " '',\n",
       " '  [SPARK-1981] updated streaming-kinesis.md',\n",
       " '  Chris Fregly <chris@fregly.com>',\n",
       " '  2014-08-17 19:33:15 -0700',\n",
       " '  Commit: 8263567, github.com/apache/spark/pull/1757',\n",
       " '',\n",
       " '  [SQL] Improve debug logging and toStrings.',\n",
       " '  Michael Armbrust <michael@databricks.com>',\n",
       " '  2014-08-17 19:00:38 -0700',\n",
       " '  Commit: 4f776df, github.com/apache/spark/pull/2004',\n",
       " '',\n",
       " '  Revert \"[SPARK-2970] [SQL] spark-sql script ends with IOException when EventLogging is enabled\"',\n",
       " '  Michael Armbrust <michael@databricks.com>',\n",
       " '  2014-08-17 18:10:45 -0700',\n",
       " '  Commit: c6a0091, github.com/apache/spark/pull/2007',\n",
       " '',\n",
       " '  SPARK-2881: Upgrade to Snappy 1.0.5.3 to avoid SPARK-2881.',\n",
       " '  Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-17 15:48:39 -0700',\n",
       " '  Commit: d411f41, github.com/apache/spark/pull/1999',\n",
       " '',\n",
       " '  [SPARK-3042] [mllib] DecisionTree Filter top-down instead of bottom-up',\n",
       " '  Joseph K. Bradley <joseph.kurata.bradley@gmail.com>',\n",
       " '  2014-08-16 23:53:14 -0700',\n",
       " '  Commit: 91af120, github.com/apache/spark/pull/1975',\n",
       " '',\n",
       " '  [SPARK-3077][MLLIB] fix some chisq-test',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-16 21:16:27 -0700',\n",
       " '  Commit: 413a329, github.com/apache/spark/pull/1982',\n",
       " '',\n",
       " '  In the stop method of ConnectionManager to cancel the ackTimeoutMonitor',\n",
       " '  GuoQiang Li <witgo@qq.com>',\n",
       " '  2014-08-16 20:05:55 -0700',\n",
       " '  Commit: f02e327, github.com/apache/spark/pull/1989',\n",
       " '',\n",
       " '  [SPARK-1065] [PySpark] improve supporting for large broadcast',\n",
       " '  Davies Liu <davies.liu@gmail.com>',\n",
       " '  2014-08-16 16:59:34 -0700',\n",
       " '  Commit: 5dd571c, github.com/apache/spark/pull/1912',\n",
       " '',\n",
       " '  [SPARK-3035] Wrong example with SparkContext.addFile',\n",
       " '  iAmGhost <kdh7807@gmail.com>',\n",
       " '  2014-08-16 16:48:38 -0700',\n",
       " '  Commit: 721f2fd, github.com/apache/spark/pull/1942',\n",
       " '',\n",
       " '  [SPARK-3081][MLLIB] rename RandomRDDGenerators to RandomRDDs',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-16 15:14:43 -0700',\n",
       " '  Commit: a12d3ae, github.com/apache/spark/pull/1979',\n",
       " '',\n",
       " '  [SPARK-3048][MLLIB] add LabeledPoint.parse and remove loadStreamingLabeledPoints',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-16 15:13:34 -0700',\n",
       " '  Commit: 0b354be, github.com/apache/spark/pull/1952',\n",
       " '',\n",
       " '  [SPARK-2677] BasicBlockFetchIterator#next can wait forever',\n",
       " '  Kousuke Saruta <sarutak@oss.nttdata.co.jp>',\n",
       " '  2014-08-16 14:15:58 -0700',\n",
       " '  Commit: bd3ce2f, github.com/apache/spark/pull/1632',\n",
       " '',\n",
       " '  [SQL] Using safe floating-point numbers in doctest',\n",
       " '  Cheng Lian <lian.cs.zju@gmail.com>',\n",
       " '  2014-08-16 11:26:51 -0700',\n",
       " '  Commit: 8c79574, github.com/apache/spark/pull/1925',\n",
       " '',\n",
       " '  [SPARK-2977] Ensure ShuffleManager is created before ShuffleBlockManager',\n",
       " '  Josh Rosen <joshrosen@apache.org>',\n",
       " '  2014-08-16 00:04:55 -0700',\n",
       " '  Commit: 0e0ec2e, github.com/apache/spark/pull/1976',\n",
       " '',\n",
       " '  [SPARK-3045] Make Serializer interface Java friendly',\n",
       " '  Reynold Xin <rxin@apache.org>',\n",
       " '  2014-08-15 23:12:34 -0700',\n",
       " '  Commit: fcf30cd, github.com/apache/spark/pull/1948',\n",
       " '',\n",
       " '  [SPARK-3015] Block on cleaning tasks to prevent Akka timeouts',\n",
       " '  Andrew Or <andrewor14@gmail.com>',\n",
       " '  2014-08-15 22:55:32 -0700',\n",
       " '  Commit: 2541537, github.com/apache/spark/pull/1931',\n",
       " '',\n",
       " \"  [SPARK-3001][MLLIB] Improve Spearman's correlation\",\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-15 21:07:55 -0700',\n",
       " '  Commit: ce06d7f, github.com/apache/spark/pull/1917',\n",
       " '',\n",
       " '  [SPARK-3078][MLLIB] Make LRWithLBFGS API consistent with others',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-15 21:04:29 -0700',\n",
       " '  Commit: c085011, github.com/apache/spark/pull/1973',\n",
       " '',\n",
       " \"  [SPARK-3046] use executor's class loader as the default serializer classloader\",\n",
       " '  Reynold Xin <rxin@apache.org>',\n",
       " '  2014-08-15 17:04:15 -0700',\n",
       " '  Commit: 077213b, github.com/apache/spark/pull/1972',\n",
       " '',\n",
       " '  [SPARK-3022] [SPARK-3041] [mllib] Call findBins once per level + unordered feature bug fix',\n",
       " '  Joseph K. Bradley <joseph.kurata.bradley@gmail.com>',\n",
       " '  2014-08-15 14:50:10 -0700',\n",
       " '  Commit: 407ea9f, github.com/apache/spark/pull/1950',\n",
       " '',\n",
       " '  SPARK-3028. sparkEventToJson should support SparkListenerExecutorMetrics...',\n",
       " '  Sandy Ryza <sandy@cloudera.com>',\n",
       " '  2014-08-15 11:35:08 -0700',\n",
       " '  Commit: 63376a0, github.com/apache/spark/pull/1961',\n",
       " '',\n",
       " '  Revert \"[SPARK-2468] Netty based block server / client module\"',\n",
       " '  Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-15 09:01:35 -0700',\n",
       " '  Commit: b066af4',\n",
       " '',\n",
       " '  [SPARK-2924] remove default args to overloaded methods',\n",
       " '  Anand Avati <avati@redhat.com>',\n",
       " '  2014-08-15 08:53:52 -0700',\n",
       " '  Commit: debb3e3, github.com/apache/spark/pull/1704',\n",
       " '',\n",
       " '  [SPARK-2468] Netty based block server / client module',\n",
       " '  Reynold Xin <rxin@apache.org>',\n",
       " '  2014-08-14 19:01:33 -0700',\n",
       " '  Commit: 3f23d2a, github.com/apache/spark/pull/1907',\n",
       " '',\n",
       " '  [SPARK-2936] Migrate Netty network module from Java to Scala',\n",
       " '  Reynold Xin <rxin@apache.org>',\n",
       " '  2014-08-10 20:36:54 -0700',\n",
       " '  Commit: d3cce58, github.com/apache/spark/pull/1865',\n",
       " '',\n",
       " '  [SPARK-2736] PySpark converter and example script for reading Avro files',\n",
       " '  Kan Zhang <kzhang@apache.org>',\n",
       " '  2014-08-14 19:03:51 -0700',\n",
       " '  Commit: 72e730e, github.com/apache/spark/pull/1916',\n",
       " '',\n",
       " '  [SPARK-3027] TaskContext: tighten visibility and provide Java friendly callback API',\n",
       " '  Reynold Xin <rxin@apache.org>',\n",
       " '  2014-08-14 18:37:02 -0700',\n",
       " '  Commit: f99e4fc, github.com/apache/spark/pull/1938',\n",
       " '',\n",
       " '  Make dev/mima runnable on Mac OS X.',\n",
       " '  Reynold Xin <rxin@apache.org>',\n",
       " '  2014-08-14 16:27:11 -0700',\n",
       " '  Commit: 475a35b, github.com/apache/spark/pull/1953',\n",
       " '',\n",
       " '  SPARK-3009: Reverted readObject method in ApplicationInfo so that Applic...',\n",
       " '  Jacek Lewandowski <lewandowski.jacek@gmail.com>',\n",
       " '  2014-08-14 15:01:39 -0700',\n",
       " '  Commit: f5d9176, github.com/apache/spark/pull/1947',\n",
       " '',\n",
       " '  Revert  [SPARK-3011][SQL] _temporary directory should be filtered out by sqlContext.parquetFile',\n",
       " '  Michael Armbrust <michael@databricks.com>',\n",
       " '  2014-08-14 13:00:21 -0700',\n",
       " '  Commit: c39a3f3, github.com/apache/spark/pull/1949',\n",
       " '',\n",
       " '  [SPARK-2979][MLlib] Improve the convergence rate by minimizing the condition number',\n",
       " '  DB Tsai <dbtsai@alpinenow.com>',\n",
       " '  2014-08-14 11:56:13 -0700',\n",
       " '  Commit: dc8ef93, github.com/apache/spark/pull/1897',\n",
       " '',\n",
       " '  Minor cleanup of metrics.Source',\n",
       " '  Reynold Xin <rxin@apache.org>',\n",
       " '  2014-08-14 11:22:41 -0700',\n",
       " '  Commit: a3dc54f, github.com/apache/spark/pull/1943',\n",
       " '',\n",
       " '  [SPARK-2925] [sql]fix spark-sql and start-thriftserver shell bugs when set --driver-java-options',\n",
       " '  wangfei <wangfei_hello@126.com>, wangfei <wangfei1@huawei.com>',\n",
       " '  2014-08-14 10:55:51 -0700',\n",
       " '  Commit: df25acd, github.com/apache/spark/pull/1851',\n",
       " '',\n",
       " '  [SQL] Python JsonRDD UTF8 Encoding Fix',\n",
       " '  Ahir Reddy <ahirreddy@gmail.com>',\n",
       " '  2014-08-14 10:48:52 -0700',\n",
       " '  Commit: 850abaa, github.com/apache/spark/pull/1914',\n",
       " '',\n",
       " '  [SPARK-2927][SQL] Add a conf to configure if we always read Binary columns stored in Parquet as String columns',\n",
       " '  Yin Huai <huai@cse.ohio-state.edu>',\n",
       " '  2014-08-14 10:46:33 -0700',\n",
       " '  Commit: de501e1, github.com/apache/spark/pull/1855',\n",
       " '',\n",
       " '  [SPARK-3011][SQL] _temporary directory should be filtered out by sqlContext.parquetFile',\n",
       " '  Chia-Yung Su <chiayung@appier.com>',\n",
       " '  2014-08-14 10:43:08 -0700',\n",
       " '  Commit: 221c84e, github.com/apache/spark/pull/1924',\n",
       " '',\n",
       " '  SPARK-2893: Do not swallow Exceptions when running a custom kryo registrator',\n",
       " '  Graham Dennis <graham.dennis@gmail.com>',\n",
       " '  2014-08-14 02:24:18 -0700',\n",
       " '  Commit: af809de, github.com/apache/spark/pull/1827',\n",
       " '',\n",
       " '  [SPARK-3029] Disable local execution of Spark jobs by default',\n",
       " '  Aaron Davidson <aaron@databricks.com>',\n",
       " '  2014-08-14 01:37:38 -0700',\n",
       " '  Commit: 0cb2b82, github.com/apache/spark/pull/1321',\n",
       " '',\n",
       " '  [SPARK-2995][MLLIB] add ALS.setIntermediateRDDStorageLevel',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-13 23:53:44 -0700',\n",
       " '  Commit: 1baf06f, github.com/apache/spark/pull/1913',\n",
       " '',\n",
       " '  [Docs] Add missing <code> tags (minor)',\n",
       " '  Andrew Or <andrewor14@gmail.com>',\n",
       " '  2014-08-13 23:24:23 -0700',\n",
       " '  Commit: bf7c6e1, github.com/apache/spark/pull/1936',\n",
       " '',\n",
       " '  [SPARK-3006] Failed to execute spark-shell in Windows OS',\n",
       " '  Masayoshi TSUZUKI <tsudukim@oss.nttdata.co.jp>',\n",
       " '  2014-08-13 22:17:07 -0700',\n",
       " '  Commit: dcd99c3, github.com/apache/spark/pull/1918',\n",
       " '',\n",
       " '  SPARK-3020: Print completed indices rather than tasks in web UI',\n",
       " '  Patrick Wendell <pwendell@gmail.com>',\n",
       " '  2014-08-13 18:08:38 -0700',\n",
       " '  Commit: c6cb55a, github.com/apache/spark/pull/1933',\n",
       " '',\n",
       " '  [SPARK-2986] [SQL] fixed: setting properties does not effect',\n",
       " '  guowei <guowei@upyoo.com>',\n",
       " '  2014-08-13 17:45:24 -0700',\n",
       " '  Commit: a8d2649, github.com/apache/spark/pull/1904',\n",
       " '',\n",
       " '  [SPARK-2970] [SQL] spark-sql script ends with IOException when EventLogging is enabled',\n",
       " '  Kousuke Saruta <sarutak@oss.nttdata.co.jp>',\n",
       " '  2014-08-13 17:42:38 -0700',\n",
       " '  Commit: b5b632c, github.com/apache/spark/pull/1891',\n",
       " '',\n",
       " '  [SPARK-2935][SQL]Fix parquet predicate push down bug',\n",
       " '  Michael Armbrust <michael@databricks.com>',\n",
       " '  2014-08-13 17:40:59 -0700',\n",
       " '  Commit: e8e7f17, github.com/apache/spark/pull/1863',\n",
       " '',\n",
       " '  [SPARK-2650][SQL] More precise initial buffer size estimation for in-memory column buffer',\n",
       " '  Cheng Lian <lian.cs.zju@gmail.com>',\n",
       " '  2014-08-13 17:37:55 -0700',\n",
       " '  Commit: ee7d2cc, github.com/apache/spark/pull/1901',\n",
       " '',\n",
       " '  [SPARK-2994][SQL] Support for udfs that take complex types',\n",
       " '  Michael Armbrust <michael@databricks.com>',\n",
       " '  2014-08-13 17:35:38 -0700',\n",
       " '  Commit: 71b8408, github.com/apache/spark/pull/1915',\n",
       " '',\n",
       " '  [SPARK-2817] [SQL] add \"show create table\" support',\n",
       " '  tianyi <tianyi@asiainfo-linkage.com>, tianyi <tianyi@asiainfo.com>, tianyi <tianyi.asiainfo@gmail.com>',\n",
       " '  2014-08-13 16:50:02 -0700',\n",
       " '  Commit: 0fb1198, github.com/apache/spark/pull/1760',\n",
       " '',\n",
       " '  [SPARK-3004][SQL] Added null checking when retrieving row set',\n",
       " '  Cheng Lian <lian.cs.zju@gmail.com>',\n",
       " '  2014-08-13 16:27:50 -0700',\n",
       " '  Commit: 8732375, github.com/apache/spark/pull/1920',\n",
       " '',\n",
       " '  [MLLIB] use Iterator.fill instead of Array.fill',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-13 16:20:49 -0700',\n",
       " '  Commit: e63bf87, github.com/apache/spark/pull/1930',\n",
       " '',\n",
       " '  [SPARK-2983] [PySpark] improve performance of sortByKey()',\n",
       " '  Davies Liu <davies.liu@gmail.com>',\n",
       " '  2014-08-13 14:57:12 -0700',\n",
       " '  Commit: a7bc21c, github.com/apache/spark/pull/1898',\n",
       " '',\n",
       " '  [SPARK-3013] [SQL] [PySpark] convert array into list',\n",
       " '  Davies Liu <davies.liu@gmail.com>',\n",
       " '  2014-08-13 14:56:11 -0700',\n",
       " '  Commit: 9936020, github.com/apache/spark/pull/1928',\n",
       " '',\n",
       " '  [SPARK-2963] [SQL] There no documentation about building to use HiveServer and CLI for SparkSQL',\n",
       " '  Kousuke Saruta <sarutak@oss.nttdata.co.jp>',\n",
       " '  2014-08-13 14:42:57 -0700',\n",
       " '  Commit: 78f2f99, github.com/apache/spark/pull/1885',\n",
       " '',\n",
       " '  [SPARK-2993] [MLLib] colStats (wrapper around MultivariateStatisticalSummary) in Statistics',\n",
       " '  Doris Xin <doris.s.xin@gmail.com>',\n",
       " '  2014-08-12 23:47:42 -0700',\n",
       " '  Commit: 5ebeb3f, github.com/apache/spark/pull/1911',\n",
       " '',\n",
       " '  [SPARK-1777 (partial)] bugfix: make size of requested memory correctly',\n",
       " '  Zhang, Liye <liye.zhang@intel.com>',\n",
       " '  2014-08-12 23:43:36 -0700',\n",
       " '  Commit: ec5e2b0, github.com/apache/spark/pull/1892',\n",
       " '',\n",
       " '  Use transferTo when copy merge files in ExternalSorter',\n",
       " '  Raymond Liu <raymond.liu@intel.com>',\n",
       " '  2014-08-12 23:19:35 -0700',\n",
       " '  Commit: be674b3, github.com/apache/spark/pull/1884',\n",
       " '',\n",
       " '  [SPARK-2953] Allow using short names for io compression codecs',\n",
       " '  Reynold Xin <rxin@apache.org>',\n",
       " '  2014-08-12 22:50:29 -0700',\n",
       " '  Commit: 837bf60, github.com/apache/spark/pull/1873',\n",
       " '',\n",
       " '  SPARK-2830 [MLlib]: re-organize mllib documentation',\n",
       " '  Ameet Talwalkar <atalwalkar@gmail.com>',\n",
       " '  2014-08-12 17:15:21 -0700',\n",
       " '  Commit: cffd9bb, github.com/apache/spark/pull/1908',\n",
       " '',\n",
       " '  fix flaky tests',\n",
       " '  Davies Liu <davies.liu@gmail.com>',\n",
       " '  2014-08-12 16:26:01 -0700',\n",
       " '  Commit: b5f8083, github.com/apache/spark/pull/1910',\n",
       " '',\n",
       " '  [MLlib] Correctly set vectorSize and alpha',\n",
       " '  Liquan Pei <liquanpei@gmail.com>',\n",
       " '  2014-08-12 00:28:00 -0700',\n",
       " '  Commit: 2a8117a, github.com/apache/spark/pull/1900',\n",
       " '',\n",
       " '  [SPARK-2923][MLLIB] Implement some basic BLAS routines',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  2014-08-11 22:33:45 -0700',\n",
       " '  Commit: 872c170, github.com/apache/spark/pull/1849',\n",
       " '',\n",
       " '  [SQL] [SPARK-2826] Reduce the memory copy while building the hashmap for HashOuterJoin',\n",
       " '  Cheng Hao <hao.cheng@intel.com>',\n",
       " '  2014-08-11 20:45:14 -0700',\n",
       " '  Commit: f66f260, github.com/apache/spark/pull/1765',\n",
       " '',\n",
       " '  [SPARK-2650][SQL] Build column buffers in smaller batches',\n",
       " '  Michael Armbrust <michael@databricks.com>',\n",
       " '  2014-08-11 20:21:56 -0700',\n",
       " '  Commit: 779d1eb, github.com/apache/spark/pull/1880',\n",
       " '',\n",
       " '  [SPARK-2968][SQL] Fix nullabilities of Explode.',\n",
       " '  Takuya UESHIN <ueshin@happy-camper.st>',\n",
       " '  2014-08-11 20:18:03 -0700',\n",
       " '  Commit: 54b387f, github.com/apache/spark/pull/1888',\n",
       " '',\n",
       " '  [SPARK-2965][SQL] Fix HashOuterJoin output nullabilities.',\n",
       " '  Takuya UESHIN <ueshin@happy-camper.st>',\n",
       " '  2014-08-11 20:15:01 -0700',\n",
       " '  Commit: dcbf079, github.com/apache/spark/pull/1887',\n",
       " '',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#它看起來怎樣？讓我們使用collect（）動作查看RDD的內容\n",
    "RDDread. collect ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spark Change Log'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#為了以有組織的格式顯示Spark RDD的內容，可以使用諸如“first（）”，“take（）”和“takeSample（False，10,2）”之類的動作。\n",
    "\n",
    "#first（） -這將返回數據集中的第一個元素 顯示changes.txt文件的第一行\n",
    "RDDread. first ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark Change Log', '----------------', '', 'Release 1.1.0', '']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take（n） - 這將返回數據集中的前n行，並在控制台上顯示它們\n",
    "RDDread. take (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  Merge pull request #530 from mbautin/master-update-log4j-and-make-compile-in-IntelliJ',\n",
       " '  Xiangrui Meng <meng@databricks.com>',\n",
       " '  Commit: d666053, github.com/apache/spark/pull/261',\n",
       " '  [SPARK-2659][SQL] Fix division semantics for hive',\n",
       " '  ca44b51 Tue Nov 5 01:32:55 2013 -0800',\n",
       " '',\n",
       " '  Commit: 71fcd2e, github.com/apache/spark/pull/1786',\n",
       " '',\n",
       " '  Reza Zadeh <rizlar@gmail.com>',\n",
       " '']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TakeSample（withReplacemen t，n，[seed]） - \n",
    "#此操作將返回數據集中的n個元素，有或沒有替換（true或false）。Seed是一個可選參數，用作隨機生成器\n",
    "\n",
    "RDDread. takeSample (False, 10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takeSample()函数和sample函数是一个原理，但是不使用相对比例采样，而是按设定的采样个数进行采样，同时返回结果不再是RDD，\n",
    "#而是相当于对采样后的数据进行collect()，返回结果的集合为单机的数组。\n",
    "#图中，左侧的方框代表分布式的各个节点上的分区，右侧方框代表单机上返回的结果数组。通过takeSample对数据采样，设置为采样一份数据，\n",
    "#返回结果为V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14577"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count（） - 知道RDD中的行數\n",
    "RDDread. count ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDD分區\n",
    "# 並行性是任何分佈式系統的關鍵特性，其中操作通過將數據劃分為多個並行分區來完成。同時對分區執行相同的操作，這有助於利用spark實現快速數據處理\n",
    "# 。通過將數據劃分為多個分區，可以在apache spark中並行地有效地應用Map和Reduce操作。RDD中每個分區的副本分佈在群集的不同節點上運行的多個工\n",
    "# 作線程中，以便在單個工作程序發生故障時RDD仍然可用。\n",
    "\n",
    "# RDD上每個操作的並行度取決於RDD具有的固定分區數。我們可以使用repartition（）和coalesce（）方法在創建或稍後指定並行度或分區數\n",
    "\n",
    "partRDD = sc.textFile(\"file:///home/hadoop/CHANGES.txt\",4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#coalesce（）是repartition（）方法的優化版本，可避免數據移動，通常用於在過濾大型數據集後減少分區數。\n",
    "\n",
    "#可以使用以下方法檢查RDD當前的分區數 - rdd.getNumPartitions（）\n",
    "partRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用reduceByKey操作處理數據時，Spark將根據默認並行性形成多個輸出分區，這取決於每個節點上可用的節點數和內核數。\n",
    "\n",
    "# 以下是兩個版本的地圖轉換，它們分別利用RDD的每個分區，分別利用火花集群的最大核心和內存 -\n",
    "\n",
    "# partRDD.mapPartitions（）：這在每個分區上單獨運行一個map操作，這與普通的map操作不同，在map操作中map用於對整個RDD的每一行進行操作。\n",
    "\n",
    "# mapPartitionsWithIndex（）：這與partRDD.mapPartitions的作用相同，但我們還可以指定必須應用此操作的分區號。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF（用戶定義的函數）\n",
    "\n",
    "# UDF提供了一種向Spark添加單獨函數的簡單方法，可以在各種轉換階段使用。UDF通常用於在Spark RDD上執行多個任務。\n",
    "\n",
    "# 讓我們用一個簡單的用例來理解使用movie數據集的上述概念。\n",
    "\n",
    "# 關於數據集：\n",
    "\n",
    "# u.user - 有關用戶的人口統計信息; 這是一個以製表符分隔的列表\n",
    "userRDD = sc.textFile(\"file:///home/hadoop//u.user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "943"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建Spark應用程序\n",
    "\n",
    "# 讓我們創建一個用戶定義的函數來將用戶劃分為年齡組\n",
    "def parse_N_calculate_age(data):\n",
    "             userid,age,gender,occupation,zip = data.split(\"|\")\n",
    "             return  userid, age_group(int(age)),gender,occupation,zip,int(age)\n",
    "def  age_group(age):\n",
    "        if age < 10 :\n",
    "           return '0-10'\n",
    "        elif age < 20:\n",
    "           return '10-20'\n",
    "        elif age < 30:\n",
    "           return '20-30'\n",
    "        elif age < 40:\n",
    "           return '30-40'\n",
    "        elif age < 50:\n",
    "           return '40-50'\n",
    "        elif age < 60:\n",
    "           return '50-60'\n",
    "        elif age < 70:\n",
    "           return '60-70'\n",
    "        elif age < 80:\n",
    "           return '70-80'\n",
    "        else :\n",
    "           return '80+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_age_bucket = userRDD.map(parse_N_calculate_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#現在，讓我們分析年齡組“20-30”進行進一步分析\n",
    "RDD_20_30 = data_with_age_bucket.filter(lambda line : '20-30' in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'administrator': 19,\n",
       " 'artist': 12,\n",
       " 'doctor': 2,\n",
       " 'educator': 12,\n",
       " 'engineer': 23,\n",
       " 'entertainment': 8,\n",
       " 'executive': 7,\n",
       " 'healthcare': 4,\n",
       " 'homemaker': 3,\n",
       " 'lawyer': 4,\n",
       " 'librarian': 11,\n",
       " 'marketing': 5,\n",
       " 'none': 2,\n",
       " 'other': 38,\n",
       " 'programmer': 30,\n",
       " 'salesman': 2,\n",
       " 'scientist': 8,\n",
       " 'student': 116,\n",
       " 'technician': 12,\n",
       " 'writer': 14}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由於我們要分析20-30歲年齡組的多個事物，我們可以將它放在內存中以便進行這些操作，這樣就可以花更少的時間進行計算。\n",
    "\n",
    "# 讓我們根據他們在給定age_group 20-30中的職業來計算用戶數量\n",
    "freq = RDD_20_30.map(lambda line : line[3]).countByValue()\n",
    "\n",
    "dict(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F': 85, 'M': 247}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#現在讓我們根據性別計算同一年齡組的電影用戶數量\n",
    "age_wise = RDD_20_30.map (lambda line : line[2]).countByValue()\n",
    "\n",
    "dict(age_wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[18] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#由於我們完成了對上述緩存數據的操作，我們可以使用unpersisit（）方法將它們從內存中刪除 -\n",
    "RDD_20_30.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 現在，我們將使用累加器在上述電影數據集中進行異常值檢測。讓我們假設任何年齡組80歲以上的人都是異常值並被標記為過度使用，任何屬於0-10歲年\n",
    "# 齡段的人都是異常值並標記為不足。\n",
    "Under_age = sc.accumulator(0)\n",
    "\n",
    "Over_age = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(data):\n",
    "    global Over_age, Under_age\n",
    "    age_grp = data[1]\n",
    "    if(age_grp == \"70-80\"):\n",
    "        Over_age +=1\n",
    "    if(age_grp == \"0-10\"):\n",
    "        Under_age +=1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用上面的方法通過一個用於計算異常值的函數傳遞整個RDD\n",
    "df = data_with_age_bucket.map(outliers).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#現在我們將檢查有多少用戶未成年以及有多少人超過年齡 -\n",
    "Under_age.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Over_age.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total user count is  943\n",
      "total movie users profession wise  {'marketing': 5, 'homemaker': 3, 'other': 38, 'educator': 12, 'engineer': 23, 'executive': 7, 'none': 2, 'salesman': 2, 'lawyer': 4, 'librarian': 11, 'administrator': 19, 'artist': 12, 'technician': 12, 'entertainment': 8, 'healthcare': 4, 'programmer': 30, 'student': 116, 'writer': 14, 'doctor': 2, 'scientist': 8}\n",
      "under age users of the movie are  1\n",
      "over age users of the movie are  4\n"
     ]
    }
   ],
   "source": [
    "#完整程式\n",
    "userRDD = sc.textFile(\"file:///home/hadoop//u.user\")\n",
    "\n",
    "def parse_N_calculate_age(data):\n",
    "             userid,age,gender,occupation,zip = data.split(\"|\")\n",
    "             return  userid, age_group(int(age)),gender,occupation,zip,int(age)\n",
    "\n",
    "def  age_group(age):\n",
    "        if age < 10 :\n",
    "           return '0-10'\n",
    "        elif age < 20:\n",
    "           return '10-20'\n",
    "        elif age < 30:\n",
    "           return '20-30'\n",
    "        elif age < 40:\n",
    "           return '30-40'\n",
    "        elif age < 50:\n",
    "           return '40-50'\n",
    "        elif age < 60:\n",
    "           return '50-60'\n",
    "        elif age < 70:\n",
    "           return '60-70'\n",
    "        elif age < 80:\n",
    "           return '70-80'\n",
    "        else :\n",
    "           return '80+'\n",
    "\n",
    "data_with_age_bucket = userRDD.map(parse_N_calculate_age)\n",
    "\n",
    "RDD_20_30 = data_with_age_bucket.filter(lambda line : '20-30' in line)\n",
    "\n",
    "freq = RDD_20_30.map(lambda line : line[3]).countByValue()\n",
    "\n",
    "print (\"total user count is \",userRDD.count())\n",
    "\n",
    "print (\"total movie users profession wise \",dict(freq))\n",
    "\n",
    "Under_age = sc.accumulator(0)\n",
    "Over_age = sc.accumulator(0)\n",
    "\n",
    "def outliers(data):\n",
    "    global Over_age, Under_age\n",
    "    age_grp = data[1]\n",
    "    if(age_grp == \"70-80\"):\n",
    "        Over_age +=1\n",
    "    if(age_grp == \"0-10\"):\n",
    "        Under_age +=1\n",
    "    return data\n",
    "\n",
    "df = data_with_age_bucket.map(outliers).collect()\n",
    "\n",
    "print (\"under age users of the movie are \",Under_age)\n",
    "print (\"over age users of the movie are \",Over_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
