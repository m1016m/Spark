{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create SparkSession \"spark01\"\n",
    "from pyspark.sql import SparkSession\n",
    "spark01 = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以 SparkSession.read.text() 讀取 text file, 轉成DataFrame \"df01\"\n",
    "df01=spark01.read.text(\"file:///home/hadoop/sample_movielens_ratings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df01.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "| 0::2::3::1424380312|\n",
      "| 0::3::1::1424380312|\n",
      "| 0::5::2::1424380312|\n",
      "| 0::9::4::1424380312|\n",
      "|0::11::1::1424380312|\n",
      "|0::12::2::1424380312|\n",
      "|0::15::1::1424380312|\n",
      "|0::17::1::1424380312|\n",
      "|0::19::1::1424380312|\n",
      "|0::21::1::1424380312|\n",
      "|0::23::1::1424380312|\n",
      "|0::26::3::1424380312|\n",
      "|0::27::1::1424380312|\n",
      "|0::28::1::1424380312|\n",
      "|0::29::1::1424380312|\n",
      "|0::30::1::1424380312|\n",
      "|0::31::1::1424380312|\n",
      "|0::34::1::1424380312|\n",
      "|0::37::1::1424380312|\n",
      "|0::41::2::1424380312|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df01.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以 DataFrame.rdd 將 DataFrame \"df01\" 轉成RDD \"rawDataRDD\"\n",
    "rawDataRDD=df01.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='0::2::3::1424380312')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawDataRDD.first() #檢視 rawDataRDD 第一筆資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以RDD.map(lambda) 將 RDD \"rawDataRDD\" 依 \"::\" 分割 map 成 RDD \"partsRDD\"\n",
    "partsRDD = rawDataRDD.map(lambda r: r.value.split(\"::\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '2', '3', '1424380312']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partsRDD.first() #['0','2','3','143......']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#資料型別轉換, 使用 Row type\n",
    "ratingsRDD = partsRDD.map(lambda x: Row(userId=int(x[0]), movieId=int(x[1]),\n",
    "                                     rating=float(x[2]), timestamp=int(x[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(movieId=2, rating=3.0, timestamp=1424380312, userId=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最後,再以SparkSeeeion.createDataFrame(RDD), 將RDD \"ratingsRDD\" 轉回成 DataFrame \"ratingsDF\"\n",
    "ratingsDF = spark01.createDataFrame(ratingsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+------+\n",
      "|movieId|rating| timestamp|userId|\n",
      "+-------+------+----------+------+\n",
      "|      2|   3.0|1424380312|     0|\n",
      "|      3|   1.0|1424380312|     0|\n",
      "|      5|   2.0|1424380312|     0|\n",
      "|      9|   4.0|1424380312|     0|\n",
      "|     11|   1.0|1424380312|     0|\n",
      "+-------+------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratingsDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#於是,我們可以用 DataFrame-Based Collaborative filtering 運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: 以DataFrame.randomSplit() 將 DataFrame \"ratingsDF\" 依設定機率比例 (8:2) 分成訓練資料 \"trainingDF\" 及 測試資料 \"testDF\"\n",
    "(trainingDF, testDF) = ratingsDF.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練資料筆數: 1183 測試資料筆數: 318\n"
     ]
    }
   ],
   "source": [
    "print('訓練資料筆數: '+str(trainingDF.count())+' 測試資料筆數: '+str(testDF.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2:  Build the recommendation model  \" alsModel\" using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\") #als, Alternating Least Squares (ALS) matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以 Estimator.fit(training Data), 訓練階段, 得到模型 alsModel\n",
    "alsModel=als.fit(trainingDF)  #alsModel, pyspark.ml.recommendation.ALSModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.recommendation.ALSModel"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alsModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3:  Evaluate the model by computing the RMSE on the test data\n",
    "#             We evaluate the recommendation model by measuring the root-mean-square error of rating prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用 Transformer.transform(test data), 預測test 資料 \"testDF\", 回傳的是 DataFrame \"predictionsDF\"\n",
    "predictionsDF = alsModel.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+------+----------+\n",
      "|movieId|rating| timestamp|userId|prediction|\n",
      "+-------+------+----------+------+----------+\n",
      "|     31|   1.0|1424380312|    13|  1.167619|\n",
      "|     31|   3.0|1424380312|     7| 1.1293797|\n",
      "|     31|   3.0|1424380312|    14| 1.7049236|\n",
      "|     31|   1.0|1424380312|     0| 1.3734546|\n",
      "|     85|   1.0|1424380312|    13| 1.2992957|\n",
      "+-------+------+----------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以 RegressionEvaluator() 建立評估器 \"evaluator01\", pyspark.ml.evaluation.RegressionEvaluator\n",
    "evaluator01 = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以 RegressionEvaluator.evaluate() 計算 rmse\n",
    "rmse = evaluator01.evaluate(predictionsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.7657409802124948\n"
     ]
    }
   ],
   "source": [
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
