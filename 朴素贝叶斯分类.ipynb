{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#朴素贝叶斯分类\n",
    "#设置Spark上下文和SparkSession\n",
    "# SparkSession的功能包括：\n",
    "# 创建DataFrame\n",
    "# 以关系型数据库中表的形式生成DataFrame，之后便可以执行SQL语句，适合小数据量的操作\n",
    "# 读取.parquet格式的文件，得到DataFrame\n",
    "\n",
    "# appName(name):\n",
    "# 给应用设置一个名称,可在Spark的web界面上显示.如果名称已占用,则会随机生成一个.\n",
    "# 界面地址默认为localhost:4040\n",
    "# (2.0版本新增)\n",
    "\n",
    "# config(key=None, value=None, conf=None)\n",
    "# 配置项,在创建SparkSession和SparkConf时都会被隐式地调用\n",
    "# (2.0版本新增)\n",
    "# 参数:\n",
    "# key ——– 配置信息的键\n",
    "# value ——– 配置信息的值\n",
    "# conf ——– SparkConf实例\n",
    "\n",
    "# getOrCreate()\n",
    "# 获取已存在的SparkSession实例,若不存在则根据构造器的配置内容创建一个\n",
    "# (2.0版本新增)\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark  Naive Bayes classification\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+\n",
      "|fixed|volatile|citric|sugar|chlorides|free|total|density|  pH|sulphates|alcohol|quality|\n",
      "+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+\n",
      "|  7.4|     0.7|   0.0|  1.9|    0.076|11.0| 34.0| 0.9978|3.51|     0.56|    9.4|      5|\n",
      "|  7.8|    0.88|   0.0|  2.6|    0.098|25.0| 67.0| 0.9968| 3.2|     0.68|    9.8|      5|\n",
      "|  7.8|    0.76|  0.04|  2.3|    0.092|15.0| 54.0|  0.997|3.26|     0.65|    9.8|      5|\n",
      "| 11.2|    0.28|  0.56|  1.9|    0.075|17.0| 60.0|  0.998|3.16|     0.58|    9.8|      6|\n",
      "|  7.4|     0.7|   0.0|  1.9|    0.076|11.0| 34.0| 0.9978|3.51|     0.56|    9.4|      5|\n",
      "+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#负载数据集\n",
    "# 负载数据集\n",
    "# 使用先前建立的DBFS掛載點來讀取數據。\n",
    "# 創建一個數據框來讀取數據。\n",
    "# option說明： 　　\n",
    "# 1、path：解析的CSV文件的目錄，路徑支持通配符； 　　\n",
    "# 2、header：默認值是false。我們知道，CSV文件第一行一般是解釋各個列的含義的名稱，如果我們不需要加載這一行，\n",
    "#    我們可以將這個選項設置為true； 　　\n",
    "# 3、delimiter：默認情況下，CSV是使用英文逗號分隔的，如果不是這個分隔，我們就可以設置這個選項。 　　\n",
    "# 4、quote：默認情況下的引號是'\"'，我們可以通過設置這個選項來支持別的引號。 　　5、mode：解析的模式。默認值是PERMISSIVE\n",
    "df = spark.read.format('com.databricks.spark.csv') \\\n",
    "            .options(header='true', inferschema='true') \\\n",
    "            .load(\"file:///home/hadoop/WineData2.csv\",header=True);\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fixed: double (nullable = true)\n",
      " |-- volatile: double (nullable = true)\n",
      " |-- citric: double (nullable = true)\n",
      " |-- sugar: double (nullable = true)\n",
      " |-- chlorides: double (nullable = true)\n",
      " |-- free: double (nullable = true)\n",
      " |-- total: double (nullable = true)\n",
      " |-- density: double (nullable = true)\n",
      " |-- pH: double (nullable = true)\n",
      " |-- sulphates: double (nullable = true)\n",
      " |-- alcohol: double (nullable = true)\n",
      " |-- quality: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()#第二部份類似jason格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+\n",
      "|fixed|volatile|citric|sugar|chlorides|free|total|density|  pH|sulphates|alcohol|quality|\n",
      "+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+\n",
      "|  7.4|     0.7|   0.0|  1.9|    0.076|11.0| 34.0| 0.9978|3.51|     0.56|    9.4|    low|\n",
      "|  7.8|    0.88|   0.0|  2.6|    0.098|25.0| 67.0| 0.9968| 3.2|     0.68|    9.8|    low|\n",
      "|  7.8|    0.76|  0.04|  2.3|    0.092|15.0| 54.0|  0.997|3.26|     0.65|    9.8|    low|\n",
      "| 11.2|    0.28|  0.56|  1.9|    0.075|17.0| 60.0|  0.998|3.16|     0.58|    9.8|    low|\n",
      "|  7.4|     0.7|   0.0|  1.9|    0.076|11.0| 34.0| 0.9978|3.51|     0.56|    9.4|    low|\n",
      "+-----+--------+------+-----+---------+----+-----+-------+----+---------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to float format\n",
    "# withColumn(colName, col)\n",
    "\n",
    "# 通过为原数据框添加一个新列或替换已存在的同名列而返回一个新数据框。colName 是一个字符串, 为新列的名字。\n",
    "# col 为这个新列的 Column 表达式。withColumn 的第一个参数必须是已存在的列的名字,　withColumn 的第二个参数必须是含有列的表达式。\n",
    "# withColumnRenamed(existing, new)\n",
    "# 重命名已存在的列并返回一个新数据框。existing 为已存在的要重命名的列, col 为新列的名字。\n",
    "def string_to_float(x):\n",
    "    return float(x)\n",
    "\n",
    "#\n",
    "def condition(r):\n",
    "    if (0<= r <= 6):\n",
    "        label = \"low\"\n",
    "    else:\n",
    "        label = \"high\"\n",
    "    return label\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "string_to_float_udf = udf(string_to_float, DoubleType())\n",
    "quality_udf = udf(lambda x: condition(x), StringType())\n",
    "\n",
    "df = df.withColumn(\"quality\", quality_udf(\"quality\"))\n",
    "\n",
    "df.show(5,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理分类数据并将数据转换为密集向量\n",
    "#监督学习版本：\n",
    "def get_dummy(df,categoricalCols,continuousCols,labelCol):\n",
    "\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "                 for c in categoricalCols ]\n",
    "\n",
    "    # default setting: dropLast=True\n",
    "#     One-Hot Encoding的处理方法\n",
    "#     在实际的机器学习的应用任务中，特征有时候并不总是连续值，有可能是一些分类值，如性别可分为“male”和“female”。在机器学习任务中，对于这样的特征，通常我们需要对其进行特征数字化，如下面的例子：\n",
    "#     性别：[\"male\"，\"female\"]\n",
    "#     地区：[\"Europe\"，\"US\"，\"Asia\"]\n",
    "#     浏览器：[\"Firefox\"，\"Chrome\"，\"Safari\"，\"Internet Explorer\"]\n",
    "#     对于某一个样本，如[\"male\"，\"US\"，\"Internet Explorer\"]，我们需要将这个分类值的特征数字化，最直接的方法，我们可以采用序列化的方式：[0,1,3]。但是这样的特征处理并不能直接放入机器学习算法中。\n",
    "#     对于上述的问题，性别的属性是二维的，同理，地区是三维的，浏览器则是思维的，这样，我们可以采用One-Hot编码的方式对上述的样本“[\"male\"，\"US\"，\"Internet Explorer\"]”编码，“male”则对应着[1，0]，同理“US”对应着[0，1，0]，“Internet Explorer”对应着[0,0,0,1]。则完整的特征数字化的结果为：[1,0,0,1,0,0,0,0,1]。\n",
    "\n",
    "# VectorAssembler是一个transformer，将多列数据转化为单列的向量列。\n",
    "\n",
    "# 转化前的数据：\n",
    "\n",
    "\n",
    "# id | hour | mobile | userFeatures     | clicked\n",
    "# ----|------|--------|------------------|---------\n",
    "#  0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0\n",
    "\n",
    "# 转化后的数据：\n",
    "\n",
    "# id | hour | mobile | userFeatures     | clicked | features\n",
    "# ----|------|--------|------------------|---------|-----------------------------\n",
    "#  0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0     | [18.0, 1.0, 0.0, 10.0, 0.5]\n",
    "\n",
    "# --------------------- \n",
    "\n",
    "# 当我们对训练集应用各种预处理操作时（特征标准化、主成分分析等等）， 我们都需要对测试集重复利用这些参数，以免出现数据泄露（data leakage）。\n",
    "\n",
    "# pipeline 实现了对全部步骤的流式化封装和管理（streaming workflows with pipelines），可以很方便地使参数集在新数据集（比如测试集）上被重复使用。\n",
    "\n",
    "# Pipeline可以将许多算法模型串联起来，比如将特征提取、归一化、分类组织在一起形成一个典型的机器学习问题工作流。主要带来两点好处：\n",
    "\n",
    "# 1. 直接调用fit和predict方法来对pipeline中的所有算法模型进行训练和预测。\n",
    "\n",
    "# 2. 可以结合grid search对参数进行选择。\n",
    "\n",
    "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
    "                 outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))\n",
    "                 for indexer in indexers ]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]\n",
    "                                + continuousCols, outputCol=\"features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "\n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "\n",
    "    data = data.withColumn('label',col(labelCol))\n",
    "\n",
    "    return data.select('features','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将数据集转换为数据帧\n",
    "from pyspark.ml.linalg import Vectors # !!!!caution: not from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def transData(data):\n",
    "     return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[7.4,0.7,0.0,1.9,...|  low|\n",
      "|[7.8,0.88,0.0,2.6...|  low|\n",
      "|[7.8,0.76,0.04,2....|  low|\n",
      "|[11.2,0.28,0.56,1...|  low|\n",
      "|[7.4,0.7,0.0,1.9,...|  low|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed = transData(df)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+\n",
      "|            features|label|indexedLabel|\n",
      "+--------------------+-----+------------+\n",
      "|[7.4,0.7,0.0,1.9,...|  low|         0.0|\n",
      "|[7.8,0.88,0.0,2.6...|  low|         0.0|\n",
      "|[7.8,0.76,0.04,2....|  low|         0.0|\n",
      "|[11.2,0.28,0.56,1...|  low|         0.0|\n",
      "|[7.4,0.7,0.0,1.9,...|  low|         0.0|\n",
      "+--------------------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#处理分类标签和变量\n",
    "# ＃索引標籤，將元數據添加到標籤列\n",
    "#​ StringIndexer是指把一组字符型标签编码成一组标签索引，索引的范围为0到标签数量，索引构建的顺序为标签的频率，优先编码频率较大的标签\n",
    "#，所以出现频率最高的标签为0号。如果输入的是数值型的，我们会把它转化成字符型，然后再对其进行编码。\n",
    "labelIndexer = StringIndexer(inputCol='label',\n",
    "                             outputCol='indexedLabel').fit(transformed)\n",
    "labelIndexer.transform(transformed).show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|            features|label|     indexedFeatures|\n",
      "+--------------------+-----+--------------------+\n",
      "|[7.4,0.7,0.0,1.9,...|  low|[7.4,0.7,0.0,1.9,...|\n",
      "|[7.8,0.88,0.0,2.6...|  low|[7.8,0.88,0.0,2.6...|\n",
      "|[7.8,0.76,0.04,2....|  low|[7.8,0.76,0.04,2....|\n",
      "|[11.2,0.28,0.56,1...|  low|[11.2,0.28,0.56,1...|\n",
      "|[7.4,0.7,0.0,1.9,...|  low|[7.4,0.7,0.0,1.9,...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ＃自動識別分類功能，並對其進行索引。\n",
    "# ＃設置maxCategories，使> 4個不同值的特徵被視為連續。\n",
    "featureIndexer =VectorIndexer(inputCol=\"features\", \\\n",
    "                              outputCol=\"indexedFeatures\", \\\n",
    "                              maxCategories=4).fit(transformed)\n",
    "featureIndexer.transform(transformed).show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+-----+\n",
      "|features                                                 |label|\n",
      "+---------------------------------------------------------+-----+\n",
      "|[4.7,0.6,0.17,2.3,0.058,17.0,106.0,0.9932,3.85,0.6,12.9] |low  |\n",
      "|[4.9,0.42,0.0,2.1,0.048,16.0,42.0,0.99154,3.71,0.74,14.0]|high |\n",
      "|[5.0,0.38,0.01,1.6,0.048,26.0,60.0,0.99084,3.7,0.75,14.0]|low  |\n",
      "|[5.0,0.4,0.5,4.3,0.046,29.0,80.0,0.9902,3.49,0.66,13.6]  |low  |\n",
      "|[5.0,0.42,0.24,2.0,0.06,19.0,50.0,0.9917,3.72,0.74,14.0] |high |\n",
      "+---------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------------------------------------------------+-----+\n",
      "|features                                                  |label|\n",
      "+----------------------------------------------------------+-----+\n",
      "|[4.6,0.52,0.15,2.1,0.054,8.0,65.0,0.9934,3.9,0.56,13.1]   |low  |\n",
      "|[5.1,0.42,0.0,1.8,0.044,18.0,88.0,0.99157,3.68,0.73,13.6] |high |\n",
      "|[5.1,0.51,0.18,2.1,0.042,16.0,101.0,0.9924,3.46,0.87,12.9]|high |\n",
      "|[5.2,0.34,0.0,1.8,0.05,27.0,63.0,0.9916,3.68,0.79,14.0]   |low  |\n",
      "|[5.3,0.715,0.19,1.5,0.161,7.0,62.0,0.99395,3.62,0.61,11.0]|low  |\n",
      "+----------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#将数据拆分为培训和测试数据集\n",
    "#＃將數據拆分為培訓和測試集（40％用於測試）\n",
    "(trainingData, testData) = transformed.randomSplit([0.6, 0.4])\n",
    "\n",
    "trainingData.show(5,False)\n",
    "testData.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#拟合朴素贝叶斯分类模型\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(featuresCol='indexedFeatures', labelCol='indexedLabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#管道体系结构\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, nb,labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------+\n",
      "|            features|label|predictedLabel|\n",
      "+--------------------+-----+--------------+\n",
      "|[4.6,0.52,0.15,2....|  low|           low|\n",
      "|[5.1,0.42,0.0,1.8...| high|           low|\n",
      "|[5.1,0.51,0.18,2....| high|           low|\n",
      "|[5.2,0.34,0.0,1.8...|  low|           low|\n",
      "|[5.3,0.715,0.19,1...|  low|           low|\n",
      "+--------------------+-----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#做出预测\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "# Select example rows to display.\n",
    "predictions.select(\"features\",\"label\",\"predictedLabel\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.29477\n"
     ]
    }
   ],
   "source": [
    "#评价\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
